{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83228cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Improved Temporal Graph Neural Network for Anti-Money Laundering Detection\n",
    "==========================================================================\n",
    "Optimized for F2 Score with structured code organization\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (precision_recall_curve, roc_auc_score, f1_score, \n",
    "                           precision_score, recall_score, fbeta_score, \n",
    "                           confusion_matrix, average_precision_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74ecce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent.parent))  # Adjust as needed\n",
    "from config import DATAPATH, SAMPLE_DATAPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ddba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f457192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration class for hyperparameters and settings\"\"\"\n",
    "    # Model architecture\n",
    "    HIDDEN_DIM = 128  # Increased from 128\n",
    "    NODE_DIM = 15\n",
    "    EDGE_DIM = 9\n",
    "    DROPOUT_RATE = 0.3\n",
    "    \n",
    "    # Training parameters\n",
    "    LEARNING_RATE = 0.0005  \n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    EPOCHS = 100\n",
    "    PATIENCE = 10  # Early stopping patience\n",
    "    \n",
    "    # F2 score optimization\n",
    "    BETA = 2  # For F2 score (emphasizes recall)\n",
    "    # CLASS_WEIGHT_MULTIPLIER = 10  # Strong emphasis on minority class\n",
    "\n",
    "    # Criterion parameters\n",
    "    FOCAL_LOSS_ALPHA = 0.25\n",
    "    FOCAL_LOSS_GAMMA = 2.0\n",
    "    \n",
    "    # Data processing\n",
    "    TIME_WINDOW = '7D'\n",
    "    VALIDATION_SPLIT = 0.17\n",
    "    TEST_SPLIT = 0.13\n",
    "    \n",
    "    # Threshold optimization\n",
    "    THRESHOLD_SEARCH_RANGE = np.arange(0.05, 0.95, 0.05)\n",
    "\n",
    "    # Chunk Size\n",
    "    CHUNK_SIZE = 7  # Number of snapshots to process before backpropagation\n",
    "\n",
    "    # Scheduler parameters\n",
    "    SCHEDULER_FACTOR = 0.5\n",
    "    SCHEDULER_PATIENCE = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc23495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB\")\n",
    "\n",
    "def detailed_memory_profile():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        # print(f\"Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB\")\n",
    "        \n",
    "        # Show memory summary\n",
    "        # print(torch.cuda.memory_summary())\n",
    "        return allocated, cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0671bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for addressing class imbalance - better than BCE for F2 optimization\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08365f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalGraphDataProcessor:\n",
    "    \"\"\"Enhanced data processor with better feature engineering for F2 optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, time_window='7D'):\n",
    "        self.time_window = time_window\n",
    "        self.scalers = {}\n",
    "        self.encoders = {}\n",
    "\n",
    "    def load_and_preprocess(self, df):\n",
    "        \"\"\"Load SAML-D dataset and perform initial preprocessing\"\"\"\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        \n",
    "        # Combine date and time into datetime\n",
    "        df['datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "        df = df.sort_values('datetime').reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Loaded {len(df)} transactions\")\n",
    "        print(f\"Suspicious transactions: {df['Is_laundering'].sum()} ({df['Is_laundering'].mean()*100:.3f}%)\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"Enhanced feature engineering for better detection\"\"\"\n",
    "        print(\"Engineering enhanced features...\")\n",
    "        \n",
    "        # Time-based features (more granular)\n",
    "        df['hour'] = df['datetime'].dt.hour.astype('int8')\n",
    "        df['month'] = df['datetime'].dt.month.astype('int8')\n",
    "        df['day_of_week'] = df['datetime'].dt.dayofweek.astype('int8')\n",
    "        df['day_of_month'] = df['datetime'].dt.day.astype('int8')\n",
    "        df['is_weekend'] = (df['day_of_week'] >= 5).astype('int8')\n",
    "        df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 5)).astype('int8')  # Night transactions\n",
    "        \n",
    "        # Amount-based features\n",
    "        df['log_amount'] = np.log1p(df['Amount']).astype('float32')\n",
    "        \n",
    "        # Calculate amount percentiles for anomaly detection\n",
    "        # amount_percentiles = df['Amount'].quantile([0.95, 0.99]).values\n",
    "        # df['high_amount'] = (df['Amount'] > amount_percentiles[0]).astype('int8')\n",
    "        # df['very_high_amount'] = (df['Amount'] > amount_percentiles[1]).astype('int8')\n",
    "        \n",
    "        # Geographic risk features\n",
    "        df['cross_border'] = (df['Payment_type'] == 'Cross-border').astype('int8')\n",
    "        risky_countries = {'Mexico', 'Turkey', 'Morocco', 'UAE'}\n",
    "        df['high_risk_sender'] = df['Sender_bank_location'].isin(risky_countries).astype('int8')\n",
    "        df['high_risk_receiver'] = df['Receiver_bank_location'].isin(risky_countries).astype('int8')\n",
    "        # df['both_high_risk'] = (df['high_risk_sender'] & df['high_risk_receiver']).astype('int8')\n",
    "        \n",
    "        # Currency features\n",
    "        df['currency_mismatch'] = (df['Payment_currency'] != df['Received_currency']).astype('int8')\n",
    "        \n",
    "        # Convert target\n",
    "        df['Is_laundering'] = df['Is_laundering'].astype('int8')\n",
    "        \n",
    "        # Clean up\n",
    "        columns_to_drop = ['Date', 'Time', 'Amount', 'Sender_bank_location', \n",
    "                          'Receiver_bank_location', 'Payment_currency', 'Received_currency', \n",
    "                          'Laundering_type']\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_temporal_snapshots(self, df, account_features):\n",
    "        \"\"\"Create temporal graph snapshots with enhanced features\"\"\"\n",
    "        print(\"Creating temporal graph snapshots...\")\n",
    "        \n",
    "        # Global account mapping\n",
    "        all_accounts = list(set(df['Sender_account'].unique()) | set(df['Receiver_account'].unique()))\n",
    "        global_account_to_idx = {acc: idx for idx, acc in enumerate(all_accounts)}\n",
    "        global_num_nodes = len(all_accounts)\n",
    "        \n",
    "        # Time windows\n",
    "        start_date = df['datetime'].min().normalize().date()\n",
    "        end_date = df['datetime'].max().normalize().date()\n",
    "        \n",
    "        snapshots = []\n",
    "        print(f\"Processing time range: {start_date} to {end_date}\")\n",
    "\n",
    "        for window_start in pd.date_range(start=start_date, end=end_date, freq=self.time_window, inclusive='left'):\n",
    "            window_end = window_start + pd.Timedelta(days=7)\n",
    "            window_start_str = pd.to_datetime(window_start).strftime('%Y-%m-%d')\n",
    "            window_end_str = pd.to_datetime(window_end).strftime('%Y-%m-%d')\n",
    "            print(f\"Processing window: {window_start_str} to {window_end_str}\")\n",
    "            \n",
    "            # Get transactions in current window\n",
    "            window_mask = (df['datetime'] >= window_start_str) & (df['datetime'] < window_end_str)\n",
    "            window_trnx_data = df[window_mask].copy()\n",
    "            \n",
    "            # Account features for this window\n",
    "            window_accounts_features = account_features[account_features['window_start'] == window_start_str]\n",
    "            \n",
    "            if len(window_trnx_data) > 0:\n",
    "                graph_data = self._create_graph_snapshot(\n",
    "                    window_trnx_data, window_accounts_features,\n",
    "                    window_start_str, global_account_to_idx, global_num_nodes\n",
    "                )\n",
    "                if graph_data is not None:\n",
    "                    snapshots.append(graph_data)\n",
    "\n",
    "        print(f\"Created {len(snapshots)} temporal snapshots\")\n",
    "        return snapshots, global_num_nodes\n",
    "\n",
    "    def _create_graph_snapshot(self, window_trnx_data, window_accounts_features, \n",
    "                              timestamp, global_account_to_idx, global_num_nodes):\n",
    "        \"\"\"Create enhanced graph snapshot\"\"\"\n",
    "        if len(window_trnx_data) == 0:\n",
    "            return None\n",
    "\n",
    "        # Enhanced edge features\n",
    "        edge_feature_columns = [\n",
    "            'Payment_type_encoded', 'log_amount', 'month', 'day_of_week', 'hour', \n",
    "            'currency_mismatch', 'cross_border', 'high_risk_sender', 'high_risk_receiver',\n",
    "        ]\n",
    "        \n",
    "        # Filter available columns\n",
    "        edge_feature_columns = [col for col in edge_feature_columns if col in window_trnx_data.columns]\n",
    "\n",
    "        # Node features\n",
    "        node_feature_columns = ['sent_txns_count', 'fan_out', 'recv_txns_count', 'fan_in', \n",
    "                               'max_sent_txn_count', 'max_recv_txn_count', 'sent_recv_ratio', \n",
    "                               'fanout_fanin_ratio', 'log_med_sent_amt', 'log_std_sent_amt', \n",
    "                               'log_med_recv_amt', 'log_std_recv_amt', 'log_max_sent_txn_amt', \n",
    "                               'log_max_recv_txn_amt', 'log_total_txns_amt']\n",
    "\n",
    "        # Create mappings and features\n",
    "        sender_mapped = window_trnx_data['Sender_account'].map(global_account_to_idx)\n",
    "        receiver_mapped = window_trnx_data['Receiver_account'].map(global_account_to_idx)\n",
    "        edge_index = np.column_stack((sender_mapped, receiver_mapped))\n",
    "        edge_features = window_trnx_data[edge_feature_columns].values\n",
    "        transaction_labels = window_trnx_data['Is_laundering'].values\n",
    "\n",
    "        # Node features\n",
    "        node_features = np.zeros((global_num_nodes, len(node_feature_columns)))\n",
    "        try:\n",
    "            window_accounts_features['global_idx'] = window_accounts_features['account'].map(global_account_to_idx)\n",
    "            node_features[window_accounts_features['global_idx'].values] = window_accounts_features[node_feature_columns].values\n",
    "        except: \n",
    "            raise ValueError(\"Error in mapping account features to global indices.\")\n",
    "\n",
    "        # Convert to tensors\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "        edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
    "        transaction_labels = torch.tensor(transaction_labels, dtype=torch.float)\n",
    "\n",
    "        return Data(\n",
    "            x=node_features,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_features,\n",
    "            y=transaction_labels,\n",
    "            timestamp=timestamp,\n",
    "            num_nodes=global_num_nodes\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e55e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal GNN Model for Edge Classification\n",
    "class TemporalEdgeClassifier(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, hidden_dim, dropout_rate):\n",
    "        super(TemporalEdgeClassifier, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.GRUCell(node_dim, hidden_dim)\n",
    "        self.gnn1 = SAGEConv(hidden_dim, hidden_dim, aggr='mean')\n",
    "        self.gnn2 = SAGEConv(hidden_dim, hidden_dim, aggr='mean')\n",
    "        self.gnn3 = SAGEConv(hidden_dim, hidden_dim, aggr='mean')\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        # self.classifier = nn.Linear(hidden_dim * 2 + edge_dim, 1)  # Binary classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + edge_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim * 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data, h):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "        \n",
    "        # Update node hidden states with RNN (using current x)\n",
    "        h = self.rnn(x, h)\n",
    "        \n",
    "        # Apply GNN layers\n",
    "        h = F.relu(self.gnn1(h, edge_index))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.gnn2(h, edge_index))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.gnn3(h, edge_index))\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        # Edge features: concat sender h, receiver h, edge_attr\n",
    "        h_i = h[edge_index[0]]\n",
    "        h_j = h[edge_index[1]]\n",
    "        edge_input = torch.cat([h_i, h_j, edge_attr], dim=-1)\n",
    "        \n",
    "        # Prediction\n",
    "        out = self.classifier(edge_input)\n",
    "        \n",
    "        return out, h  # Return logits and updated h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59dd2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"Enhanced trainer class optimized for F2 score\"\"\"\n",
    "    \n",
    "    def __init__(self, config=Config(), mem_profile=False):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        self.mem_profile = mem_profile\n",
    "\n",
    "    def find_optimal_threshold(self, probs, labels):\n",
    "        \"\"\"Find optimal threshold for F2 score\"\"\"\n",
    "        best_f2 = 0\n",
    "        best_threshold = 0.5\n",
    "        \n",
    "        for threshold in self.config.THRESHOLD_SEARCH_RANGE:\n",
    "            preds = (probs >= threshold).astype(int)\n",
    "            f2 = fbeta_score(labels, preds, beta=self.config.BETA, average='binary', zero_division=0)\n",
    "            if f2 > best_f2:\n",
    "                best_f2 = f2\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        return best_threshold, best_f2\n",
    "    \n",
    "    def compute_class_weights(self, snapshots):\n",
    "        \"\"\"Compute class weights for focal loss\"\"\"\n",
    "        all_labels = []\n",
    "        for snap in snapshots:\n",
    "            all_labels.extend(snap.y.cpu().numpy())\n",
    "        \n",
    "        all_labels = np.array(all_labels)\n",
    "        pos_weight = len(all_labels) / (2 * np.sum(all_labels))\n",
    "        return torch.tensor(pos_weight, dtype=torch.float).to(self.device)\n",
    "    \n",
    "    def train_model(self, snapshots, global_num_nodes):\n",
    "        \"\"\"Enhanced training with F2 optimization\"\"\"\n",
    "        \n",
    "        # Split data\n",
    "        train_size = int(len(snapshots) * (1 - self.config.VALIDATION_SPLIT - self.config.TEST_SPLIT))\n",
    "        val_size = int(len(snapshots) * self.config.VALIDATION_SPLIT)\n",
    "        \n",
    "        train_snaps = snapshots[:train_size]\n",
    "        val_snaps = snapshots[train_size:train_size + val_size]\n",
    "        test_snaps = snapshots[train_size + val_size:]\n",
    "        \n",
    "        print(f\"Data split - Train: {len(train_snaps)}, Val: {len(val_snaps)}, Test: {len(test_snaps)}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        model = TemporalEdgeClassifier(\n",
    "            self.config.NODE_DIM, \n",
    "            self.config.EDGE_DIM, \n",
    "            self.config.HIDDEN_DIM,\n",
    "            self.config.DROPOUT_RATE\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Compute class weights for focal loss\n",
    "        pos_weight = self.compute_class_weights(train_snaps)\n",
    "        criterion = FocalLoss(alpha=self.config.FOCAL_LOSS_ALPHA, gamma=self.config.FOCAL_LOSS_GAMMA)\n",
    "        \n",
    "        # Optimizer with different learning rates for different components\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': model.rnn.parameters(), 'lr': self.config.LEARNING_RATE * 0.5},\n",
    "            {'params': model.gnn1.parameters(), 'lr': self.config.LEARNING_RATE},\n",
    "            {'params': model.gnn2.parameters(), 'lr': self.config.LEARNING_RATE},\n",
    "            {'params': model.gnn3.parameters(), 'lr': self.config.LEARNING_RATE},\n",
    "            {'params': model.classifier.parameters(), 'lr': self.config.LEARNING_RATE * 1.5}\n",
    "        ], weight_decay=self.config.WEIGHT_DECAY)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=self.config.SCHEDULER_FACTOR, \n",
    "            patience=self.config.SCHEDULER_PATIENCE, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        best_f2_score = 0\n",
    "        patience_counter = 0\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "        f2_history = []\n",
    "        \n",
    "        for epoch in range(self.config.EPOCHS):\n",
    "            # Chunk size\n",
    "            k_steps = self.config.CHUNK_SIZE\n",
    "            \n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            # Initialize hidden state at the start of each epoch\n",
    "            h = torch.zeros(global_num_nodes, self.config.HIDDEN_DIM).to(self.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            chunk_loss = 0.0\n",
    "\n",
    "            if self.mem_profile:\n",
    "                print(f\"=== EPOCH {epoch} START ===\")\n",
    "                epoch_start_mem = detailed_memory_profile()\n",
    "                print(f\"Epoch Mem Allocated: {epoch_start_mem[0]:.3f} GB, Cached: {epoch_start_mem[0]:.3f} GB\")\n",
    "\n",
    "            for i, snap in enumerate(train_snaps):\n",
    "                snap = snap.to(self.device)\n",
    "                out, h = model(snap, h)      # Forward pass\n",
    "                loss = criterion(out.squeeze(), snap.y)  # Loss computation\n",
    "                chunk_loss += loss\n",
    "\n",
    "                # Backpropagation every k_steps\n",
    "                if (i + 1) % k_steps == 0 or (i + 1) == len(train_snaps):\n",
    "                    chunk_loss /= k_steps\n",
    "                    # Backward pass\n",
    "                    chunk_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Gradient clipping\n",
    "                    optimizer.step()\n",
    "                    # Detach h\n",
    "                    h = h.detach()\n",
    "                    # Reset for next chunk\n",
    "                    optimizer.zero_grad()\n",
    "                    chunk_loss = 0.0\n",
    "\n",
    "                # Total epoch loss\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_snaps)\n",
    "            train_loss_history.append(avg_train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_probs_list, val_labels_list = [], []\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                h = torch.zeros(global_num_nodes, self.config.HIDDEN_DIM).to(self.device)\n",
    "                for snap in val_snaps:\n",
    "                    snap = snap.to(self.device)\n",
    "                    out, h = model(snap, h)\n",
    "                    loss = criterion(out.squeeze(), snap.y)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    preds = torch.sigmoid(out).squeeze()\n",
    "                    val_probs_list.append(preds.cpu())\n",
    "                    val_labels_list.append(snap.y.cpu())\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_snaps)\n",
    "            val_loss_history.append(avg_val_loss)\n",
    "            \n",
    "            # Calculate F2 score with optimal threshold\n",
    "            val_probs = torch.cat(val_probs_list).numpy()\n",
    "            val_labels = torch.cat(val_labels_list).numpy()\n",
    "            \n",
    "            optimal_threshold, f2_score = self.find_optimal_threshold(val_probs, val_labels)\n",
    "            f2_history.append(f2_score)\n",
    "            recall = recall_score(val_labels, (val_probs >= optimal_threshold).astype(int), zero_division=0)\n",
    "            \n",
    "            # Update scheduler with F2 score\n",
    "            # scheduler.step(f2_score)\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Early stopping based on F2 score\n",
    "            if f2_score > best_f2_score:\n",
    "                best_f2_score = f2_score\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                # torch.save(model.state_dict(), './outputs/best_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # if (epoch + 1) % 10 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch+1}: Train Loss(x1e3): {1000*avg_train_loss:.4f}, Val Loss(x1e3): {1000*avg_val_loss:.4f}, \"\n",
    "                        f\"F2: {f2_score:.4f}, Threshold: {optimal_threshold:.3f}, Recall: {recall:.4f}, \"\n",
    "                        f\"LR: {current_lr:.6f}\")\n",
    "\n",
    "            if patience_counter >= self.config.PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "        \n",
    "        # Load best model and evaluate\n",
    "        # model.load_state_dict(torch.load('./outputs/best_model.pth'))\n",
    "        \n",
    "        # Final evaluation\n",
    "        results = self._evaluate_model(model, train_snaps, val_snaps, test_snaps, global_num_nodes)\n",
    "        results.update({\n",
    "            'train_loss_history': train_loss_history,\n",
    "            'val_loss_history': val_loss_history,\n",
    "            'f2_history': f2_history,\n",
    "            'model': model\n",
    "        })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _evaluate_model(self, model, train_snaps, val_snaps, test_snaps, global_num_nodes):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        model.eval()\n",
    "        results = {}\n",
    "        \n",
    "        for split_name, snaps in [('val', val_snaps), ('test', test_snaps)]:\n",
    "            probs_list, labels_list = [], []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                h = torch.zeros(global_num_nodes, self.config.HIDDEN_DIM).to(self.device)\n",
    "                for snap in snaps:\n",
    "                    snap = snap.to(self.device)\n",
    "                    out, h = model(snap, h)\n",
    "                    preds = torch.sigmoid(out).squeeze().cpu().numpy()\n",
    "                    probs_list.extend(preds)\n",
    "                    labels_list.extend(snap.y.cpu().numpy())\n",
    "            \n",
    "            probs = np.array(probs_list)\n",
    "            labels = np.array(labels_list)\n",
    "            \n",
    "            # Find optimal threshold\n",
    "            optimal_threshold, best_f2 = self.find_optimal_threshold(probs, labels)\n",
    "            binary_preds = (probs >= optimal_threshold).astype(int)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            precision = precision_score(labels, binary_preds, zero_division=0)\n",
    "            recall = recall_score(labels, binary_preds, zero_division=0)\n",
    "            f1 = f1_score(labels, binary_preds, zero_division=0)\n",
    "            roc_auc = roc_auc_score(labels, probs)\n",
    "            pr_auc = average_precision_score(labels, probs)\n",
    "            \n",
    "            results[f'{split_name}_probs'] = probs\n",
    "            results[f'{split_name}_labels'] = labels\n",
    "            results[f'{split_name}_threshold'] = optimal_threshold\n",
    "            results[f'{split_name}_precision'] = precision\n",
    "            results[f'{split_name}_recall'] = recall\n",
    "            results[f'{split_name}_f1'] = f1\n",
    "            results[f'{split_name}_f2'] = best_f2\n",
    "            results[f'{split_name}_roc_auc'] = roc_auc\n",
    "            results[f'{split_name}_pr_auc'] = pr_auc\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2124555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire dataset\n",
    "df = pd.read_csv(DATAPATH)\n",
    "\n",
    "# Filter by data range\n",
    "# df = df[df['Date'] < '2023-08-18']\n",
    "# df = df.head(300000).copy()\n",
    "\n",
    "# run feature engg.ipynb to get the account_stats_7D.csv\n",
    "account_stats = pd.read_csv('../account_stats_7D.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9178de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Loaded 9504852 transactions\n",
      "Suspicious transactions: 9873 (0.104%)\n",
      "Engineering enhanced features...\n"
     ]
    }
   ],
   "source": [
    "graph_processor = TemporalGraphDataProcessor()\n",
    "df = graph_processor.load_and_preprocess(df)\n",
    "df = graph_processor.engineer_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8bfb9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# For each categorical column\n",
    "# categorical_cols = ['Payment_currency', 'Received_currency', 'Sender_bank_location', \n",
    "#                    'Receiver_bank_location', 'Payment_type']\n",
    "categorical_cols = ['Payment_type']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "    \n",
    "# Drop original object columns\n",
    "df = df.drop(categorical_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43c740f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process accont_stats\n",
    "columns = ['med_sent_amt', 'std_sent_amt', 'med_recv_amt', 'std_recv_amt', \n",
    "           'max_sent_txn_amt', 'max_recv_txn_amt', 'total_txns_amt']\n",
    "\n",
    "for col in columns:\n",
    "    account_stats['log_' + col] = np.log1p(account_stats[col]).astype('float32')\n",
    "\n",
    "account_stats = account_stats.drop(columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "094d1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data types to optimize memory\n",
    "account_stats = account_stats.astype({\n",
    "    'sent_txns_count': 'int32',\n",
    "    'recv_txns_count': 'int32',\n",
    "    'fan_out': 'int32',\n",
    "    'fan_in': 'int32',\n",
    "    'max_sent_txn_count': 'int32',\n",
    "    'max_recv_txn_count': 'int32',\n",
    "    'sent_recv_ratio': 'float32',\n",
    "    'fanout_fanin_ratio': 'float32'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79bf8e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporal graph snapshots...\n",
      "Processing time range: 2022-10-07 to 2023-08-23\n",
      "Processing window: 2022-10-07 to 2022-10-14\n",
      "Processing window: 2022-10-14 to 2022-10-21\n",
      "Processing window: 2022-10-21 to 2022-10-28\n",
      "Processing window: 2022-10-28 to 2022-11-04\n",
      "Processing window: 2022-11-04 to 2022-11-11\n",
      "Processing window: 2022-11-11 to 2022-11-18\n",
      "Processing window: 2022-11-18 to 2022-11-25\n",
      "Processing window: 2022-11-25 to 2022-12-02\n",
      "Processing window: 2022-12-02 to 2022-12-09\n",
      "Processing window: 2022-12-09 to 2022-12-16\n",
      "Processing window: 2022-12-16 to 2022-12-23\n",
      "Processing window: 2022-12-23 to 2022-12-30\n",
      "Processing window: 2022-12-30 to 2023-01-06\n",
      "Processing window: 2023-01-06 to 2023-01-13\n",
      "Processing window: 2023-01-13 to 2023-01-20\n",
      "Processing window: 2023-01-20 to 2023-01-27\n",
      "Processing window: 2023-01-27 to 2023-02-03\n",
      "Processing window: 2023-02-03 to 2023-02-10\n",
      "Processing window: 2023-02-10 to 2023-02-17\n",
      "Processing window: 2023-02-17 to 2023-02-24\n",
      "Processing window: 2023-02-24 to 2023-03-03\n",
      "Processing window: 2023-03-03 to 2023-03-10\n",
      "Processing window: 2023-03-10 to 2023-03-17\n",
      "Processing window: 2023-03-17 to 2023-03-24\n",
      "Processing window: 2023-03-24 to 2023-03-31\n",
      "Processing window: 2023-03-31 to 2023-04-07\n",
      "Processing window: 2023-04-07 to 2023-04-14\n",
      "Processing window: 2023-04-14 to 2023-04-21\n",
      "Processing window: 2023-04-21 to 2023-04-28\n",
      "Processing window: 2023-04-28 to 2023-05-05\n",
      "Processing window: 2023-05-05 to 2023-05-12\n",
      "Processing window: 2023-05-12 to 2023-05-19\n",
      "Processing window: 2023-05-19 to 2023-05-26\n",
      "Processing window: 2023-05-26 to 2023-06-02\n",
      "Processing window: 2023-06-02 to 2023-06-09\n",
      "Processing window: 2023-06-09 to 2023-06-16\n",
      "Processing window: 2023-06-16 to 2023-06-23\n",
      "Processing window: 2023-06-23 to 2023-06-30\n",
      "Processing window: 2023-06-30 to 2023-07-07\n",
      "Processing window: 2023-07-07 to 2023-07-14\n",
      "Processing window: 2023-07-14 to 2023-07-21\n",
      "Processing window: 2023-07-21 to 2023-07-28\n",
      "Processing window: 2023-07-28 to 2023-08-04\n",
      "Processing window: 2023-08-04 to 2023-08-11\n",
      "Processing window: 2023-08-11 to 2023-08-18\n",
      "Processing window: 2023-08-18 to 2023-08-25\n",
      "Created 46 temporal snapshots\n"
     ]
    }
   ],
   "source": [
    "snapshots, global_num_nodes = graph_processor.create_temporal_snapshots(df, account_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a083f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 3.13GB, Cached: 7.21GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89fc6783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Data split - Train: 32, Val: 7, Test: 7\n",
      "Epoch 1: Train Loss(x1e3): 10.3803, Val Loss(x1e3): 2.5453, F2: 0.0062, Threshold: 0.250, Recall: 0.1440, LR: 0.000250\n",
      "Epoch 2: Train Loss(x1e3): 1.4283, Val Loss(x1e3): 0.7426, F2: 0.0090, Threshold: 0.150, Recall: 0.0295, LR: 0.000250\n",
      "Epoch 3: Train Loss(x1e3): 0.8470, Val Loss(x1e3): 0.9792, F2: 0.0087, Threshold: 0.050, Recall: 0.0754, LR: 0.000250\n",
      "Epoch 4: Train Loss(x1e3): 1.0543, Val Loss(x1e3): 1.0210, F2: 0.0086, Threshold: 0.050, Recall: 0.0384, LR: 0.000250\n",
      "Epoch 5: Train Loss(x1e3): 1.0255, Val Loss(x1e3): 0.9082, F2: 0.0100, Threshold: 0.050, Recall: 0.1125, LR: 0.000250\n",
      "Epoch 6: Train Loss(x1e3): 0.9092, Val Loss(x1e3): 0.8032, F2: 0.0087, Threshold: 0.100, Recall: 0.0267, LR: 0.000250\n",
      "Epoch 7: Train Loss(x1e3): 0.8159, Val Loss(x1e3): 0.7452, F2: 0.0088, Threshold: 0.100, Recall: 0.0741, LR: 0.000250\n",
      "Epoch 8: Train Loss(x1e3): 0.7686, Val Loss(x1e3): 0.7223, F2: 0.0091, Threshold: 0.100, Recall: 0.1344, LR: 0.000250\n",
      "Epoch 9: Train Loss(x1e3): 0.7514, Val Loss(x1e3): 0.7158, F2: 0.0090, Threshold: 0.100, Recall: 0.1845, LR: 0.000250\n",
      "Epoch 10: Train Loss(x1e3): 0.7447, Val Loss(x1e3): 0.7130, F2: 0.0087, Threshold: 0.100, Recall: 0.2119, LR: 0.000250\n",
      "Epoch 11: Train Loss(x1e3): 0.7418, Val Loss(x1e3): 0.7082, F2: 0.0088, Threshold: 0.100, Recall: 0.2257, LR: 0.000250\n",
      "Epoch 12: Train Loss(x1e3): 0.7358, Val Loss(x1e3): 0.7000, F2: 0.0091, Threshold: 0.100, Recall: 0.2277, LR: 0.000250\n",
      "Epoch 13: Train Loss(x1e3): 0.7239, Val Loss(x1e3): 0.6898, F2: 0.0097, Threshold: 0.100, Recall: 0.2236, LR: 0.000250\n",
      "Epoch 14: Train Loss(x1e3): 0.7165, Val Loss(x1e3): 0.6794, F2: 0.0103, Threshold: 0.100, Recall: 0.2147, LR: 0.000250\n",
      "Epoch 15: Train Loss(x1e3): 0.7048, Val Loss(x1e3): 0.6697, F2: 0.0107, Threshold: 0.100, Recall: 0.2044, LR: 0.000250\n",
      "Epoch 16: Train Loss(x1e3): 0.6988, Val Loss(x1e3): 0.6599, F2: 0.0115, Threshold: 0.150, Recall: 0.0302, LR: 0.000250\n",
      "Epoch 17: Train Loss(x1e3): 0.6870, Val Loss(x1e3): 0.6483, F2: 0.0122, Threshold: 0.100, Recall: 0.2263, LR: 0.000250\n",
      "Epoch 18: Train Loss(x1e3): 0.6749, Val Loss(x1e3): 0.6358, F2: 0.0137, Threshold: 0.150, Recall: 0.0370, LR: 0.000250\n",
      "Epoch 19: Train Loss(x1e3): 0.6646, Val Loss(x1e3): 0.6233, F2: 0.0148, Threshold: 0.100, Recall: 0.3059, LR: 0.000250\n",
      "Epoch 20: Train Loss(x1e3): 0.6528, Val Loss(x1e3): 0.6101, F2: 0.0167, Threshold: 0.100, Recall: 0.3457, LR: 0.000250\n",
      "Epoch 21: Train Loss(x1e3): 0.6393, Val Loss(x1e3): 0.5969, F2: 0.0192, Threshold: 0.100, Recall: 0.3711, LR: 0.000250\n",
      "Epoch 22: Train Loss(x1e3): 0.6261, Val Loss(x1e3): 0.5842, F2: 0.0222, Threshold: 0.100, Recall: 0.4170, LR: 0.000250\n",
      "Epoch 23: Train Loss(x1e3): 0.6144, Val Loss(x1e3): 0.5721, F2: 0.0253, Threshold: 0.100, Recall: 0.4822, LR: 0.000250\n",
      "Epoch 24: Train Loss(x1e3): 0.6033, Val Loss(x1e3): 0.5605, F2: 0.0285, Threshold: 0.100, Recall: 0.5281, LR: 0.000250\n",
      "Epoch 25: Train Loss(x1e3): 0.5910, Val Loss(x1e3): 0.5493, F2: 0.0328, Threshold: 0.150, Recall: 0.1859, LR: 0.000250\n",
      "Epoch 26: Train Loss(x1e3): 0.5806, Val Loss(x1e3): 0.5384, F2: 0.0365, Threshold: 0.150, Recall: 0.2243, LR: 0.000250\n",
      "Epoch 27: Train Loss(x1e3): 0.5703, Val Loss(x1e3): 0.5278, F2: 0.0400, Threshold: 0.150, Recall: 0.2545, LR: 0.000250\n",
      "Epoch 28: Train Loss(x1e3): 0.5609, Val Loss(x1e3): 0.5171, F2: 0.0446, Threshold: 0.150, Recall: 0.2936, LR: 0.000250\n",
      "Epoch 29: Train Loss(x1e3): 0.5497, Val Loss(x1e3): 0.5066, F2: 0.0478, Threshold: 0.150, Recall: 0.3155, LR: 0.000250\n",
      "Epoch 30: Train Loss(x1e3): 0.5405, Val Loss(x1e3): 0.4961, F2: 0.0600, Threshold: 0.200, Recall: 0.0741, LR: 0.000250\n",
      "Epoch 31: Train Loss(x1e3): 0.5335, Val Loss(x1e3): 0.4855, F2: 0.0700, Threshold: 0.200, Recall: 0.0850, LR: 0.000250\n",
      "Epoch 32: Train Loss(x1e3): 0.5231, Val Loss(x1e3): 0.4738, F2: 0.0881, Threshold: 0.200, Recall: 0.1008, LR: 0.000250\n",
      "Epoch 33: Train Loss(x1e3): 0.5151, Val Loss(x1e3): 0.4614, F2: 0.1022, Threshold: 0.200, Recall: 0.1159, LR: 0.000250\n",
      "Epoch 34: Train Loss(x1e3): 0.5025, Val Loss(x1e3): 0.4475, F2: 0.1236, Threshold: 0.200, Recall: 0.1392, LR: 0.000250\n",
      "Epoch 35: Train Loss(x1e3): 0.4917, Val Loss(x1e3): 0.4334, F2: 0.1535, Threshold: 0.200, Recall: 0.1818, LR: 0.000250\n",
      "Epoch 36: Train Loss(x1e3): 0.4823, Val Loss(x1e3): 0.4190, F2: 0.1968, Threshold: 0.200, Recall: 0.2490, LR: 0.000250\n",
      "Epoch 37: Train Loss(x1e3): 0.4705, Val Loss(x1e3): 0.4044, F2: 0.2275, Threshold: 0.200, Recall: 0.3141, LR: 0.000250\n",
      "Epoch 38: Train Loss(x1e3): 0.4578, Val Loss(x1e3): 0.3903, F2: 0.2606, Threshold: 0.200, Recall: 0.3896, LR: 0.000250\n",
      "Epoch 39: Train Loss(x1e3): 0.4445, Val Loss(x1e3): 0.3767, F2: 0.2790, Threshold: 0.200, Recall: 0.4595, LR: 0.000250\n",
      "Epoch 40: Train Loss(x1e3): 0.4387, Val Loss(x1e3): 0.3644, F2: 0.2892, Threshold: 0.200, Recall: 0.5267, LR: 0.000250\n",
      "Epoch 41: Train Loss(x1e3): 0.4284, Val Loss(x1e3): 0.3524, F2: 0.3171, Threshold: 0.200, Recall: 0.5940, LR: 0.000250\n",
      "Epoch 42: Train Loss(x1e3): 0.4165, Val Loss(x1e3): 0.3414, F2: 0.3293, Threshold: 0.200, Recall: 0.6337, LR: 0.000250\n",
      "Epoch 43: Train Loss(x1e3): 0.4060, Val Loss(x1e3): 0.3346, F2: 0.3354, Threshold: 0.250, Recall: 0.4225, LR: 0.000250\n",
      "Epoch 44: Train Loss(x1e3): 0.3937, Val Loss(x1e3): 0.3248, F2: 0.3673, Threshold: 0.250, Recall: 0.4746, LR: 0.000250\n",
      "Epoch 45: Train Loss(x1e3): 0.3786, Val Loss(x1e3): 0.3192, F2: 0.3838, Threshold: 0.250, Recall: 0.5281, LR: 0.000250\n",
      "Epoch 46: Train Loss(x1e3): 0.3697, Val Loss(x1e3): 0.3232, F2: 0.3797, Threshold: 0.250, Recall: 0.6008, LR: 0.000250\n",
      "Epoch 47: Train Loss(x1e3): 0.3652, Val Loss(x1e3): 0.3121, F2: 0.4004, Threshold: 0.250, Recall: 0.6235, LR: 0.000250\n",
      "Epoch 48: Train Loss(x1e3): 0.3511, Val Loss(x1e3): 0.3050, F2: 0.4193, Threshold: 0.250, Recall: 0.6598, LR: 0.000250\n",
      "Epoch 49: Train Loss(x1e3): 0.3429, Val Loss(x1e3): 0.3007, F2: 0.4367, Threshold: 0.300, Recall: 0.4726, LR: 0.000250\n",
      "Epoch 50: Train Loss(x1e3): 0.3331, Val Loss(x1e3): 0.2921, F2: 0.4619, Threshold: 0.300, Recall: 0.5192, LR: 0.000250\n",
      "Epoch 51: Train Loss(x1e3): 0.3258, Val Loss(x1e3): 0.2870, F2: 0.4876, Threshold: 0.300, Recall: 0.5782, LR: 0.000250\n",
      "Epoch 52: Train Loss(x1e3): 0.3185, Val Loss(x1e3): 0.2767, F2: 0.5082, Threshold: 0.300, Recall: 0.6001, LR: 0.000250\n",
      "Epoch 53: Train Loss(x1e3): 0.3092, Val Loss(x1e3): 0.2694, F2: 0.5337, Threshold: 0.300, Recall: 0.6461, LR: 0.000250\n",
      "Epoch 54: Train Loss(x1e3): 0.3013, Val Loss(x1e3): 0.2682, F2: 0.5329, Threshold: 0.300, Recall: 0.6783, LR: 0.000250\n",
      "Epoch 55: Train Loss(x1e3): 0.2921, Val Loss(x1e3): 0.2651, F2: 0.5310, Threshold: 0.350, Recall: 0.5439, LR: 0.000250\n",
      "Epoch 56: Train Loss(x1e3): 0.2826, Val Loss(x1e3): 0.2502, F2: 0.5612, Threshold: 0.350, Recall: 0.5809, LR: 0.000250\n",
      "Epoch 57: Train Loss(x1e3): 0.2756, Val Loss(x1e3): 0.2411, F2: 0.5849, Threshold: 0.350, Recall: 0.6104, LR: 0.000250\n",
      "Epoch 58: Train Loss(x1e3): 0.2651, Val Loss(x1e3): 0.2477, F2: 0.6019, Threshold: 0.350, Recall: 0.6687, LR: 0.000250\n",
      "Epoch 59: Train Loss(x1e3): 0.2591, Val Loss(x1e3): 0.2236, F2: 0.6225, Threshold: 0.350, Recall: 0.6722, LR: 0.000250\n",
      "Epoch 60: Train Loss(x1e3): 0.2525, Val Loss(x1e3): 0.2299, F2: 0.6196, Threshold: 0.350, Recall: 0.7003, LR: 0.000250\n",
      "Epoch 61: Train Loss(x1e3): 0.2471, Val Loss(x1e3): 0.2114, F2: 0.6369, Threshold: 0.350, Recall: 0.7058, LR: 0.000250\n",
      "Epoch 62: Train Loss(x1e3): 0.2343, Val Loss(x1e3): 0.2075, F2: 0.6545, Threshold: 0.350, Recall: 0.7311, LR: 0.000250\n",
      "Epoch 63: Train Loss(x1e3): 0.2296, Val Loss(x1e3): 0.2027, F2: 0.6744, Threshold: 0.400, Recall: 0.6866, LR: 0.000250\n",
      "Epoch 64: Train Loss(x1e3): 0.2171, Val Loss(x1e3): 0.1765, F2: 0.6921, Threshold: 0.350, Recall: 0.7339, LR: 0.000250\n",
      "Epoch 65: Train Loss(x1e3): 0.2100, Val Loss(x1e3): 0.2120, F2: 0.6851, Threshold: 0.400, Recall: 0.7442, LR: 0.000250\n",
      "Epoch 66: Train Loss(x1e3): 0.2148, Val Loss(x1e3): 0.1679, F2: 0.7089, Threshold: 0.350, Recall: 0.7305, LR: 0.000250\n",
      "Epoch 67: Train Loss(x1e3): 0.1981, Val Loss(x1e3): 0.1841, F2: 0.7020, Threshold: 0.400, Recall: 0.7428, LR: 0.000250\n",
      "Epoch 68: Train Loss(x1e3): 0.2005, Val Loss(x1e3): 0.1708, F2: 0.7126, Threshold: 0.400, Recall: 0.7387, LR: 0.000250\n",
      "Epoch 69: Train Loss(x1e3): 0.1842, Val Loss(x1e3): 0.1697, F2: 0.7094, Threshold: 0.350, Recall: 0.7970, LR: 0.000250\n",
      "Epoch 70: Train Loss(x1e3): 0.1888, Val Loss(x1e3): 0.1627, F2: 0.7289, Threshold: 0.400, Recall: 0.7476, LR: 0.000250\n",
      "Epoch 71: Train Loss(x1e3): 0.1738, Val Loss(x1e3): 0.1694, F2: 0.7135, Threshold: 0.400, Recall: 0.7551, LR: 0.000250\n",
      "Epoch 72: Train Loss(x1e3): 0.1847, Val Loss(x1e3): 0.1558, F2: 0.7350, Threshold: 0.400, Recall: 0.7476, LR: 0.000250\n",
      "Epoch 73: Train Loss(x1e3): 0.1707, Val Loss(x1e3): 0.1533, F2: 0.7357, Threshold: 0.350, Recall: 0.8107, LR: 0.000250\n",
      "Epoch 74: Train Loss(x1e3): 0.1695, Val Loss(x1e3): 0.1497, F2: 0.7437, Threshold: 0.400, Recall: 0.7682, LR: 0.000250\n",
      "Epoch 75: Train Loss(x1e3): 0.1539, Val Loss(x1e3): 0.1703, F2: 0.7277, Threshold: 0.400, Recall: 0.7984, LR: 0.000250\n",
      "Epoch 76: Train Loss(x1e3): 0.1781, Val Loss(x1e3): 0.1457, F2: 0.7489, Threshold: 0.350, Recall: 0.7599, LR: 0.000250\n",
      "Epoch 77: Train Loss(x1e3): 0.1642, Val Loss(x1e3): 0.1690, F2: 0.7442, Threshold: 0.450, Recall: 0.7599, LR: 0.000250\n",
      "Epoch 78: Train Loss(x1e3): 0.1647, Val Loss(x1e3): 0.1454, F2: 0.7545, Threshold: 0.300, Recall: 0.7778, LR: 0.000250\n",
      "Epoch 79: Train Loss(x1e3): 0.1619, Val Loss(x1e3): 0.1948, F2: 0.7337, Threshold: 0.450, Recall: 0.7963, LR: 0.000250\n",
      "Epoch 80: Train Loss(x1e3): 0.1771, Val Loss(x1e3): 0.1618, F2: 0.7401, Threshold: 0.250, Recall: 0.7586, LR: 0.000250\n",
      "Epoch 81: Train Loss(x1e3): 0.1826, Val Loss(x1e3): 0.1535, F2: 0.7630, Threshold: 0.400, Recall: 0.7908, LR: 0.000250\n",
      "Epoch 82: Train Loss(x1e3): 0.1607, Val Loss(x1e3): 0.1526, F2: 0.7312, Threshold: 0.400, Recall: 0.7723, LR: 0.000250\n",
      "Epoch 83: Train Loss(x1e3): 0.1505, Val Loss(x1e3): 0.1477, F2: 0.7464, Threshold: 0.400, Recall: 0.7922, LR: 0.000250\n",
      "Epoch 84: Train Loss(x1e3): 0.1447, Val Loss(x1e3): 0.1563, F2: 0.7579, Threshold: 0.400, Recall: 0.8182, LR: 0.000250\n",
      "Epoch 85: Train Loss(x1e3): 0.1511, Val Loss(x1e3): 0.1361, F2: 0.7606, Threshold: 0.350, Recall: 0.7716, LR: 0.000250\n",
      "Epoch 86: Train Loss(x1e3): 0.1450, Val Loss(x1e3): 0.1452, F2: 0.7753, Threshold: 0.450, Recall: 0.7757, LR: 0.000250\n",
      "Epoch 87: Train Loss(x1e3): 0.1375, Val Loss(x1e3): 0.1308, F2: 0.7730, Threshold: 0.400, Recall: 0.7929, LR: 0.000250\n",
      "Epoch 88: Train Loss(x1e3): 0.1330, Val Loss(x1e3): 0.1352, F2: 0.7741, Threshold: 0.450, Recall: 0.7798, LR: 0.000250\n",
      "Epoch 89: Train Loss(x1e3): 0.1364, Val Loss(x1e3): 0.1231, F2: 0.7922, Threshold: 0.400, Recall: 0.7942, LR: 0.000250\n",
      "Epoch 90: Train Loss(x1e3): 0.1328, Val Loss(x1e3): 0.1269, F2: 0.7939, Threshold: 0.400, Recall: 0.8080, LR: 0.000250\n",
      "Epoch 91: Train Loss(x1e3): 0.1256, Val Loss(x1e3): 0.1225, F2: 0.7926, Threshold: 0.400, Recall: 0.8059, LR: 0.000250\n",
      "Epoch 92: Train Loss(x1e3): 0.1229, Val Loss(x1e3): 0.1289, F2: 0.7832, Threshold: 0.400, Recall: 0.8114, LR: 0.000250\n",
      "Epoch 93: Train Loss(x1e3): 0.1301, Val Loss(x1e3): 0.1264, F2: 0.7815, Threshold: 0.400, Recall: 0.7771, LR: 0.000250\n",
      "Epoch 94: Train Loss(x1e3): 0.1244, Val Loss(x1e3): 0.1255, F2: 0.7985, Threshold: 0.400, Recall: 0.8114, LR: 0.000250\n",
      "Epoch 95: Train Loss(x1e3): 0.1272, Val Loss(x1e3): 0.1148, F2: 0.7988, Threshold: 0.400, Recall: 0.7908, LR: 0.000250\n",
      "Epoch 96: Train Loss(x1e3): 0.1187, Val Loss(x1e3): 0.1346, F2: 0.7971, Threshold: 0.450, Recall: 0.8121, LR: 0.000250\n",
      "Epoch 97: Train Loss(x1e3): 0.1263, Val Loss(x1e3): 0.1210, F2: 0.7960, Threshold: 0.300, Recall: 0.7997, LR: 0.000250\n",
      "Epoch 98: Train Loss(x1e3): 0.1241, Val Loss(x1e3): 0.1319, F2: 0.8095, Threshold: 0.450, Recall: 0.8141, LR: 0.000250\n",
      "Epoch 99: Train Loss(x1e3): 0.1209, Val Loss(x1e3): 0.1189, F2: 0.7962, Threshold: 0.350, Recall: 0.7970, LR: 0.000250\n",
      "Epoch 100: Train Loss(x1e3): 0.1209, Val Loss(x1e3): 0.1209, F2: 0.8097, Threshold: 0.450, Recall: 0.8066, LR: 0.000250\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(config=Config(), mem_profile=False)\n",
    "results = trainer.train_model(snapshots, global_num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb0d7038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_probs': array([0.10488342, 0.08059462, 0.01080714, ..., 0.00483381, 0.0088752 ,\n",
       "        0.00361212], shape=(1458820,), dtype=float32),\n",
       " 'val_labels': array([0., 0., 0., ..., 0., 0., 0.], shape=(1458820,), dtype=float32),\n",
       " 'val_threshold': np.float64(0.45),\n",
       " 'val_precision': 0.8223776223776224,\n",
       " 'val_recall': 0.8065843621399177,\n",
       " 'val_f1': 0.814404432132964,\n",
       " 'val_f2': 0.8096942990911594,\n",
       " 'val_roc_auc': 0.9957938164308076,\n",
       " 'val_pr_auc': 0.8463123053659317,\n",
       " 'test_probs': array([0.00309314, 0.00997286, 0.00222352, ..., 0.00175051, 0.22687027,\n",
       "        0.02475397], shape=(1384809,), dtype=float32),\n",
       " 'test_labels': array([0., 0., 0., ..., 0., 0., 0.], shape=(1384809,), dtype=float32),\n",
       " 'test_threshold': np.float64(0.45),\n",
       " 'test_precision': 0.8437900128040973,\n",
       " 'test_recall': 0.8031687995124924,\n",
       " 'test_f1': 0.82297845769591,\n",
       " 'test_f2': 0.8109771105094757,\n",
       " 'test_roc_auc': 0.9948063945342674,\n",
       " 'test_pr_auc': 0.8530219008073929,\n",
       " 'train_loss_history': [0.010380300140241161,\n",
       "  0.0014283383370639058,\n",
       "  0.0008470034481433686,\n",
       "  0.0010543340158619685,\n",
       "  0.0010255047327518696,\n",
       "  0.0009092338150367141,\n",
       "  0.000815922258880164,\n",
       "  0.0007686489780098782,\n",
       "  0.0007514098360843491,\n",
       "  0.0007446593681379454,\n",
       "  0.0007417839751724387,\n",
       "  0.0007357989043157431,\n",
       "  0.0007238799043989275,\n",
       "  0.0007164640182963922,\n",
       "  0.0007047877579680062,\n",
       "  0.0006988032710069092,\n",
       "  0.0006870086526760133,\n",
       "  0.0006748904415871948,\n",
       "  0.0006645829653280089,\n",
       "  0.0006527648138217046,\n",
       "  0.000639273793240136,\n",
       "  0.0006260618683882058,\n",
       "  0.0006144462140582618,\n",
       "  0.0006032904448147747,\n",
       "  0.0005909501669520978,\n",
       "  0.0005806422368550557,\n",
       "  0.000570252003853966,\n",
       "  0.0005609229192486964,\n",
       "  0.0005497034771906328,\n",
       "  0.0005405291149145341,\n",
       "  0.0005335180494512315,\n",
       "  0.0005230845690675778,\n",
       "  0.0005150660081199021,\n",
       "  0.0005025491300330032,\n",
       "  0.0004917131409456488,\n",
       "  0.00048234916266665095,\n",
       "  0.0004705373203250929,\n",
       "  0.00045778709136357065,\n",
       "  0.0004445009217306506,\n",
       "  0.0004386691971376422,\n",
       "  0.00042843973369599553,\n",
       "  0.00041648759361123666,\n",
       "  0.00040596615053800633,\n",
       "  0.00039373866320602247,\n",
       "  0.00037864306887058774,\n",
       "  0.00036968257882108446,\n",
       "  0.0003652169789347681,\n",
       "  0.00035109927603116375,\n",
       "  0.00034290362054889556,\n",
       "  0.0003331455750412715,\n",
       "  0.00032579466551396763,\n",
       "  0.0003185355972163961,\n",
       "  0.00030915671686670976,\n",
       "  0.00030126916635708767,\n",
       "  0.0002920705146607361,\n",
       "  0.0002826050481417042,\n",
       "  0.00027560482112676254,\n",
       "  0.00026514938781474484,\n",
       "  0.0002591164707155258,\n",
       "  0.00025254930142182275,\n",
       "  0.00024710952766326955,\n",
       "  0.00023433951355400495,\n",
       "  0.0002295532440257375,\n",
       "  0.0002171162086597178,\n",
       "  0.00021003948813813622,\n",
       "  0.00021477082509591128,\n",
       "  0.00019814130246231798,\n",
       "  0.00020054225888088695,\n",
       "  0.00018422904122417094,\n",
       "  0.00018876244212151505,\n",
       "  0.00017380526946908503,\n",
       "  0.0001847405153512227,\n",
       "  0.00017067938892978418,\n",
       "  0.00016947317340054724,\n",
       "  0.00015386480095003208,\n",
       "  0.00017809484256758878,\n",
       "  0.00016419979419879382,\n",
       "  0.00016467857221869053,\n",
       "  0.00016190203905352973,\n",
       "  0.00017705858272165642,\n",
       "  0.00018264655204802693,\n",
       "  0.00016068434274529864,\n",
       "  0.00015046244971017586,\n",
       "  0.0001446734813725925,\n",
       "  0.00015108597995094897,\n",
       "  0.0001450407605716464,\n",
       "  0.0001374652724734915,\n",
       "  0.00013296760562298005,\n",
       "  0.00013643483521263988,\n",
       "  0.00013275523974698444,\n",
       "  0.00012562847132358002,\n",
       "  0.0001229302494039075,\n",
       "  0.00013008683549742273,\n",
       "  0.00012444199205674522,\n",
       "  0.00012715357706838404,\n",
       "  0.00011868433807649126,\n",
       "  0.00012626877582988527,\n",
       "  0.00012405443590068899,\n",
       "  0.00012085784806004085,\n",
       "  0.00012091214648535242],\n",
       " 'val_loss_history': [0.002545295249936836,\n",
       "  0.0007426348913993154,\n",
       "  0.0009792428131082229,\n",
       "  0.0010210047060224628,\n",
       "  0.0009082047701148051,\n",
       "  0.0008032390448663916,\n",
       "  0.0007452208498891975,\n",
       "  0.0007223234029619821,\n",
       "  0.0007158188465317446,\n",
       "  0.000713019972733621,\n",
       "  0.0007081542613117822,\n",
       "  0.0006999826790498835,\n",
       "  0.0006898191890546254,\n",
       "  0.0006794398068450391,\n",
       "  0.0006697015570742744,\n",
       "  0.0006599396749931787,\n",
       "  0.0006482668354042939,\n",
       "  0.0006358169235422143,\n",
       "  0.0006232561011399541,\n",
       "  0.0006101042506218489,\n",
       "  0.0005968870370582278,\n",
       "  0.0005842427822894283,\n",
       "  0.0005720930639654398,\n",
       "  0.0005604664537323904,\n",
       "  0.0005493364962083953,\n",
       "  0.0005384221190719732,\n",
       "  0.0005277775427592653,\n",
       "  0.0005170644102950714,\n",
       "  0.0005066462950448372,\n",
       "  0.0004961058023452226,\n",
       "  0.0004854851868003607,\n",
       "  0.00047384753790018815,\n",
       "  0.00046138188086583147,\n",
       "  0.00044748004126761643,\n",
       "  0.0004333937623804169,\n",
       "  0.0004189753048454544,\n",
       "  0.000404422142310068,\n",
       "  0.0003903378965333104,\n",
       "  0.00037672048451245895,\n",
       "  0.00036435055413416455,\n",
       "  0.00035244624762396725,\n",
       "  0.00034144024747157734,\n",
       "  0.00033457476196677556,\n",
       "  0.00032477830037740726,\n",
       "  0.0003192119225527027,\n",
       "  0.00032323492424828667,\n",
       "  0.00031212067031966787,\n",
       "  0.00030503672522692274,\n",
       "  0.0003006651220078181,\n",
       "  0.00029209106287453324,\n",
       "  0.00028703908901661634,\n",
       "  0.00027666860010607967,\n",
       "  0.00026943080177131506,\n",
       "  0.00026818559022753367,\n",
       "  0.00026505311793049,\n",
       "  0.0002501675023397963,\n",
       "  0.0002410538644263787,\n",
       "  0.00024770341613995176,\n",
       "  0.00022355790133588016,\n",
       "  0.00022987181312471096,\n",
       "  0.00021142074131473367,\n",
       "  0.00020745275625293807,\n",
       "  0.00020267135654908737,\n",
       "  0.00017651360394665971,\n",
       "  0.00021196774261105538,\n",
       "  0.00016789305872017785,\n",
       "  0.00018410903638661175,\n",
       "  0.00017082038643171212,\n",
       "  0.00016967310615915006,\n",
       "  0.0001626986964505964,\n",
       "  0.00016944819175738042,\n",
       "  0.00015581649807115485,\n",
       "  0.00015326125971374234,\n",
       "  0.0001496826423265572,\n",
       "  0.00017031702944742783,\n",
       "  0.00014568582680242668,\n",
       "  0.00016896936514448107,\n",
       "  0.00014536049483077868,\n",
       "  0.0001947701260048364,\n",
       "  0.00016183736235169427,\n",
       "  0.00015353560073500766,\n",
       "  0.0001525948422827891,\n",
       "  0.00014767026212731644,\n",
       "  0.00015632231017142267,\n",
       "  0.00013607929057408391,\n",
       "  0.00014522263102532764,\n",
       "  0.0001307694921186859,\n",
       "  0.00013515838093423684,\n",
       "  0.00012305029044260403,\n",
       "  0.00012692906810635968,\n",
       "  0.00012250717034995823,\n",
       "  0.0001288820432299482,\n",
       "  0.00012635035610791029,\n",
       "  0.00012549777498601804,\n",
       "  0.00011481472600800251,\n",
       "  0.00013455994589354044,\n",
       "  0.00012099383145271401,\n",
       "  0.00013192287847882004,\n",
       "  0.00011889428839952285,\n",
       "  0.00012088187889146087],\n",
       " 'f2_history': [0.006223217937092157,\n",
       "  0.009003727124251435,\n",
       "  0.00867973361108481,\n",
       "  0.008636111282462526,\n",
       "  0.009976761445900402,\n",
       "  0.008722490606548578,\n",
       "  0.008762108747505232,\n",
       "  0.009073990055647633,\n",
       "  0.00900255686001526,\n",
       "  0.008723047477091414,\n",
       "  0.008788050388383747,\n",
       "  0.00911366830639494,\n",
       "  0.009659659956264853,\n",
       "  0.010265593534971893,\n",
       "  0.01073463830032492,\n",
       "  0.011536444677503933,\n",
       "  0.012187285337587804,\n",
       "  0.013676425894033025,\n",
       "  0.014812157925500823,\n",
       "  0.01669792004876853,\n",
       "  0.01924595692605426,\n",
       "  0.02218200922304594,\n",
       "  0.025280131183383438,\n",
       "  0.028454022733655566,\n",
       "  0.0328325660285922,\n",
       "  0.036518359689091395,\n",
       "  0.040012942191544436,\n",
       "  0.04461121534292266,\n",
       "  0.04775251738814492,\n",
       "  0.06004002668445631,\n",
       "  0.07004066877541798,\n",
       "  0.08808724832214765,\n",
       "  0.10216418812719139,\n",
       "  0.12364478011938117,\n",
       "  0.15346305304609684,\n",
       "  0.19679063211536377,\n",
       "  0.2274985098350884,\n",
       "  0.26059827491282805,\n",
       "  0.2789806795469687,\n",
       "  0.2892437481169027,\n",
       "  0.31712318734436795,\n",
       "  0.32934131736526945,\n",
       "  0.3354389021999564,\n",
       "  0.3673036093418259,\n",
       "  0.38377192982456143,\n",
       "  0.37971391417425226,\n",
       "  0.40036997885835096,\n",
       "  0.41928172942817293,\n",
       "  0.4367393509127789,\n",
       "  0.4619233585550403,\n",
       "  0.4875650665124349,\n",
       "  0.5081891044256012,\n",
       "  0.5336505778382054,\n",
       "  0.53292380644466,\n",
       "  0.531003080219633,\n",
       "  0.5611501258778323,\n",
       "  0.5849106203995794,\n",
       "  0.6019261637239165,\n",
       "  0.6225384322195401,\n",
       "  0.6196140308289841,\n",
       "  0.6369150779896013,\n",
       "  0.654549920176839,\n",
       "  0.674437407357499,\n",
       "  0.6921086675291074,\n",
       "  0.6850612451067054,\n",
       "  0.7088658146964856,\n",
       "  0.7019704433497537,\n",
       "  0.7125843588725684,\n",
       "  0.7094017094017094,\n",
       "  0.728901966029156,\n",
       "  0.713452566096423,\n",
       "  0.7349966284558328,\n",
       "  0.735715174903523,\n",
       "  0.7436918990703851,\n",
       "  0.72768192048012,\n",
       "  0.7488510408218437,\n",
       "  0.7442235357334767,\n",
       "  0.7544910179640718,\n",
       "  0.7336956521739131,\n",
       "  0.7400963597430407,\n",
       "  0.7629698253043938,\n",
       "  0.7311688311688311,\n",
       "  0.7464133385032958,\n",
       "  0.7579415501905972,\n",
       "  0.7606490872210954,\n",
       "  0.7752947628187551,\n",
       "  0.7730373144309215,\n",
       "  0.7741013071895425,\n",
       "  0.7921740320153235,\n",
       "  0.7939075347081817,\n",
       "  0.7926335671883432,\n",
       "  0.7832362288135594,\n",
       "  0.7814871016691958,\n",
       "  0.7984611231101512,\n",
       "  0.7988083691284468,\n",
       "  0.7970916924734078,\n",
       "  0.79601310759148,\n",
       "  0.8094653573376978,\n",
       "  0.7962176236809648,\n",
       "  0.8096942990911594],\n",
       " 'model': TemporalEdgeClassifier(\n",
       "   (rnn): GRUCell(15, 128)\n",
       "   (gnn1): SAGEConv(128, 128, aggr=mean)\n",
       "   (gnn2): SAGEConv(128, 128, aggr=mean)\n",
       "   (gnn3): SAGEConv(128, 128, aggr=mean)\n",
       "   (dropout): Dropout(p=0.3, inplace=False)\n",
       "   (classifier): Sequential(\n",
       "     (0): Linear(in_features=265, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Dropout(p=0.3, inplace=False)\n",
       "     (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82e25fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Function to compute and print confusion matrix\n",
    "def compute_confusion_matrix(labels, preds, threshold=0.5):\n",
    "\n",
    "    # Convert probabilities to binary predictions using the threshold\n",
    "    binary_preds = (preds >= threshold).astype(int)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(labels, binary_preds)\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Optional: Extract and print TP, TN, FP, FN\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"False Negatives (FN): {fn}\")\n",
    "    print(f\"True Positives (TP): {tp}\")\n",
    "    print(f\"Precision: {tp / (tp + fp + 1e-8):.4f}\")\n",
    "    print(f\"Recall: {tp / (tp + fn + 1e-8):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51d19dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = results['test_probs']\n",
    "test_labels = results['test_labels']\n",
    "val_probs = results['val_probs']\n",
    "val_labels = results['val_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f1f36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1457108     254]\n",
      " [    282    1176]]\n",
      "True Negatives (TN): 1457108\n",
      "False Positives (FP): 254\n",
      "False Negatives (FN): 282\n",
      "True Positives (TP): 1176\n",
      "Precision: 0.8224\n",
      "Recall: 0.8066\n"
     ]
    }
   ],
   "source": [
    "compute_confusion_matrix(val_labels, val_probs, threshold=0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c31c4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1382924     244]\n",
      " [    323    1318]]\n",
      "True Negatives (TN): 1382924\n",
      "False Positives (FP): 244\n",
      "False Negatives (FN): 323\n",
      "True Positives (TP): 1318\n",
      "Precision: 0.8438\n",
      "Recall: 0.8032\n"
     ]
    }
   ],
   "source": [
    "compute_confusion_matrix(test_labels, test_probs, threshold=0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b1255f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR AUC Score: 0.8530\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate PR curve data\n",
    "precision, recall, thresholds = precision_recall_curve(test_labels, test_probs)\n",
    "pr_auc = average_precision_score(test_labels, test_probs)\n",
    "\n",
    "# Plot PR curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, linewidth=2, label=f'PR AUC = {pr_auc:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"PR AUC Score: {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5373b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ae20c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-backup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
