{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83228cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Improved Temporal Graph Neural Network for Anti-Money Laundering Detection\n",
    "==========================================================================\n",
    "Optimized for F2 Score with structured code organization\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (precision_recall_curve, roc_auc_score, f1_score, \n",
    "                           precision_score, recall_score, fbeta_score, \n",
    "                           confusion_matrix, average_precision_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74ecce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent.parent))  # Adjust as needed\n",
    "from config import DATAPATH, SAMPLE_DATAPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ddba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f457192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration class for hyperparameters and settings\"\"\"\n",
    "    # Model architecture\n",
    "    HIDDEN_DIM = 128  # Increased from 128\n",
    "    NODE_DIM = 15\n",
    "    EDGE_DIM = 9\n",
    "    DROPOUT_RATE = 0.3\n",
    "    \n",
    "    # Training parameters\n",
    "    LEARNING_RATE = 0.0005  \n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    EPOCHS = 125\n",
    "    PATIENCE = 10  # Early stopping patience\n",
    "    \n",
    "    # F2 score optimization\n",
    "    BETA = 2  # For F2 score (emphasizes recall)\n",
    "    # CLASS_WEIGHT_MULTIPLIER = 10  # Strong emphasis on minority class\n",
    "\n",
    "    # Criterion parameters\n",
    "    FOCAL_LOSS_ALPHA = 0.25\n",
    "    FOCAL_LOSS_GAMMA = 2.0\n",
    "    \n",
    "    # Data processing\n",
    "    TIME_WINDOW = '7D'\n",
    "    VALIDATION_SPLIT = 0.17\n",
    "    TEST_SPLIT = 0.13\n",
    "    \n",
    "    # Threshold optimization\n",
    "    THRESHOLD_SEARCH_RANGE = np.arange(0.05, 0.95, 0.05)\n",
    "\n",
    "    # Chunk Size\n",
    "    CHUNK_SIZE = 4  # Number of snapshots to process before backpropagation\n",
    "\n",
    "    # Scheduler parameters\n",
    "    SCHEDULER_FACTOR = 0.5\n",
    "    SCHEDULER_PATIENCE = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc23495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB\")\n",
    "\n",
    "def detailed_memory_profile():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        # print(f\"Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB\")\n",
    "        \n",
    "        # Show memory summary\n",
    "        # print(torch.cuda.memory_summary())\n",
    "        return allocated, cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0671bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for addressing class imbalance - better than BCE for F2 optimization\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08365f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalGraphDataProcessor:\n",
    "    \"\"\"Enhanced data processor with better feature engineering for F2 optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, time_window='7D'):\n",
    "        self.time_window = time_window\n",
    "        self.scalers = {}\n",
    "        self.encoders = {}\n",
    "\n",
    "    def load_and_preprocess(self, df):\n",
    "        \"\"\"Load SAML-D dataset and perform initial preprocessing\"\"\"\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        \n",
    "        # Combine date and time into datetime\n",
    "        df['datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "        df = df.sort_values('datetime').reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Loaded {len(df)} transactions\")\n",
    "        print(f\"Suspicious transactions: {df['Is_laundering'].sum()} ({df['Is_laundering'].mean()*100:.3f}%)\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"Enhanced feature engineering for better detection\"\"\"\n",
    "        print(\"Engineering enhanced features...\")\n",
    "        \n",
    "        # Time-based features (more granular)\n",
    "        df['hour'] = df['datetime'].dt.hour.astype('int8')\n",
    "        df['month'] = df['datetime'].dt.month.astype('int8')\n",
    "        df['day_of_week'] = df['datetime'].dt.dayofweek.astype('int8')\n",
    "        df['day_of_month'] = df['datetime'].dt.day.astype('int8')\n",
    "        df['is_weekend'] = (df['day_of_week'] >= 5).astype('int8')\n",
    "        df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 5)).astype('int8')  # Night transactions\n",
    "        \n",
    "        # Amount-based features\n",
    "        df['log_amount'] = np.log1p(df['Amount']).astype('float32')\n",
    "        \n",
    "        # Calculate amount percentiles for anomaly detection\n",
    "        # amount_percentiles = df['Amount'].quantile([0.95, 0.99]).values\n",
    "        # df['high_amount'] = (df['Amount'] > amount_percentiles[0]).astype('int8')\n",
    "        # df['very_high_amount'] = (df['Amount'] > amount_percentiles[1]).astype('int8')\n",
    "        \n",
    "        # Geographic risk features\n",
    "        df['cross_border'] = (df['Payment_type'] == 'Cross-border').astype('int8')\n",
    "        risky_countries = {'Mexico', 'Turkey', 'Morocco', 'UAE'}\n",
    "        df['high_risk_sender'] = df['Sender_bank_location'].isin(risky_countries).astype('int8')\n",
    "        df['high_risk_receiver'] = df['Receiver_bank_location'].isin(risky_countries).astype('int8')\n",
    "        # df['both_high_risk'] = (df['high_risk_sender'] & df['high_risk_receiver']).astype('int8')\n",
    "        \n",
    "        # Currency features\n",
    "        df['currency_mismatch'] = (df['Payment_currency'] != df['Received_currency']).astype('int8')\n",
    "        \n",
    "        # Convert target\n",
    "        df['Is_laundering'] = df['Is_laundering'].astype('int8')\n",
    "        \n",
    "        # Clean up\n",
    "        columns_to_drop = ['Date', 'Time', 'Amount', 'Sender_bank_location', \n",
    "                          'Receiver_bank_location', 'Payment_currency', 'Received_currency', \n",
    "                          'Laundering_type']\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_temporal_snapshots(self, df, account_features):\n",
    "        \"\"\"Create temporal graph snapshots with enhanced features\"\"\"\n",
    "        print(\"Creating temporal graph snapshots...\")\n",
    "        \n",
    "        # Global account mapping\n",
    "        all_accounts = list(set(df['Sender_account'].unique()) | set(df['Receiver_account'].unique()))\n",
    "        global_account_to_idx = {acc: idx for idx, acc in enumerate(all_accounts)}\n",
    "        global_num_nodes = len(all_accounts)\n",
    "        \n",
    "        # Time windows\n",
    "        start_date = df['datetime'].min().normalize().date()\n",
    "        end_date = df['datetime'].max().normalize().date()\n",
    "        \n",
    "        snapshots = []\n",
    "        print(f\"Processing time range: {start_date} to {end_date}\")\n",
    "\n",
    "        for window_start in pd.date_range(start=start_date, end=end_date, freq=self.time_window, inclusive='left'):\n",
    "            window_end = window_start + pd.Timedelta(days=7)\n",
    "            window_start_str = pd.to_datetime(window_start).strftime('%Y-%m-%d')\n",
    "            window_end_str = pd.to_datetime(window_end).strftime('%Y-%m-%d')\n",
    "            print(f\"Processing window: {window_start_str} to {window_end_str}\")\n",
    "            \n",
    "            # Get transactions in current window\n",
    "            window_mask = (df['datetime'] >= window_start_str) & (df['datetime'] < window_end_str)\n",
    "            window_trnx_data = df[window_mask].copy()\n",
    "            \n",
    "            # Account features for this window\n",
    "            window_accounts_features = account_features[account_features['window_start'] == window_start_str]\n",
    "            \n",
    "            if len(window_trnx_data) > 0:\n",
    "                graph_data = self._create_graph_snapshot(\n",
    "                    window_trnx_data, window_accounts_features,\n",
    "                    window_start_str, global_account_to_idx, global_num_nodes\n",
    "                )\n",
    "                if graph_data is not None:\n",
    "                    snapshots.append(graph_data)\n",
    "\n",
    "        print(f\"Created {len(snapshots)} temporal snapshots\")\n",
    "        return snapshots, global_num_nodes\n",
    "\n",
    "    def _create_graph_snapshot(self, window_trnx_data, window_accounts_features, \n",
    "                              timestamp, global_account_to_idx, global_num_nodes):\n",
    "        \"\"\"Create enhanced graph snapshot\"\"\"\n",
    "        if len(window_trnx_data) == 0:\n",
    "            return None\n",
    "\n",
    "        # Enhanced edge features\n",
    "        edge_feature_columns = [\n",
    "            'Payment_type_encoded', 'log_amount', 'month', 'day_of_week', 'hour', \n",
    "            'currency_mismatch', 'cross_border', 'high_risk_sender', 'high_risk_receiver',\n",
    "        ]\n",
    "        \n",
    "        # Filter available columns\n",
    "        edge_feature_columns = [col for col in edge_feature_columns if col in window_trnx_data.columns]\n",
    "\n",
    "        # Node features\n",
    "        node_feature_columns = ['sent_txns_count', 'fan_out', 'recv_txns_count', 'fan_in', \n",
    "                               'max_sent_txn_count', 'max_recv_txn_count', 'sent_recv_ratio', \n",
    "                               'fanout_fanin_ratio', 'log_med_sent_amt', 'log_std_sent_amt', \n",
    "                               'log_med_recv_amt', 'log_std_recv_amt', 'log_max_sent_txn_amt', \n",
    "                               'log_max_recv_txn_amt', 'log_total_txns_amt']\n",
    "\n",
    "        # Create mappings and features\n",
    "        sender_mapped = window_trnx_data['Sender_account'].map(global_account_to_idx)\n",
    "        receiver_mapped = window_trnx_data['Receiver_account'].map(global_account_to_idx)\n",
    "        edge_index = np.column_stack((sender_mapped, receiver_mapped))\n",
    "        edge_features = window_trnx_data[edge_feature_columns].values\n",
    "        transaction_labels = window_trnx_data['Is_laundering'].values\n",
    "\n",
    "        # Node features\n",
    "        node_features = np.zeros((global_num_nodes, len(node_feature_columns)))\n",
    "        try:\n",
    "            window_accounts_features['global_idx'] = window_accounts_features['account'].map(global_account_to_idx)\n",
    "            node_features[window_accounts_features['global_idx'].values] = window_accounts_features[node_feature_columns].values\n",
    "        except: \n",
    "            raise ValueError(\"Error in mapping account features to global indices.\")\n",
    "\n",
    "        # Convert to tensors\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "        edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
    "        transaction_labels = torch.tensor(transaction_labels, dtype=torch.float)\n",
    "\n",
    "        return Data(\n",
    "            x=node_features,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_features,\n",
    "            y=transaction_labels,\n",
    "            timestamp=timestamp,\n",
    "            num_nodes=global_num_nodes\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e55e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal GNN Model for Edge Classification\n",
    "class TemporalEdgeClassifier(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, hidden_dim, dropout_rate):\n",
    "        super(TemporalEdgeClassifier, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.GRUCell(node_dim, hidden_dim)\n",
    "        self.gnn1 = SAGEConv(hidden_dim, hidden_dim, aggr='mean')\n",
    "        self.gnn2 = SAGEConv(hidden_dim, hidden_dim, aggr='mean')\n",
    "        self.gnn3 = SAGEConv(hidden_dim, hidden_dim, aggr='mean')\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        # self.classifier = nn.Linear(hidden_dim * 2 + edge_dim, 1)  # Binary classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + edge_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim * 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data, h):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "        \n",
    "        # Update node hidden states with RNN (using current x)\n",
    "        h = self.rnn(x, h)\n",
    "        \n",
    "        # Apply GNN layers\n",
    "        h = F.relu(self.gnn1(h, edge_index))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.gnn2(h, edge_index))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.gnn3(h, edge_index))\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        # Edge features: concat sender h, receiver h, edge_attr\n",
    "        h_i = h[edge_index[0]]\n",
    "        h_j = h[edge_index[1]]\n",
    "        edge_input = torch.cat([h_i, h_j, edge_attr], dim=-1)\n",
    "        \n",
    "        # Prediction\n",
    "        out = self.classifier(edge_input)\n",
    "        \n",
    "        return out, h  # Return logits and updated h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59dd2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"Enhanced trainer class optimized for F2 score\"\"\"\n",
    "    \n",
    "    def __init__(self, config=Config(), mem_profile=False):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        self.mem_profile = mem_profile\n",
    "\n",
    "    def find_optimal_threshold(self, probs, labels):\n",
    "        \"\"\"Find optimal threshold for F2 score\"\"\"\n",
    "        best_f2 = 0\n",
    "        best_threshold = 0.5\n",
    "        \n",
    "        for threshold in self.config.THRESHOLD_SEARCH_RANGE:\n",
    "            preds = (probs >= threshold).astype(int)\n",
    "            f2 = fbeta_score(labels, preds, beta=self.config.BETA, average='binary', zero_division=0)\n",
    "            if f2 > best_f2:\n",
    "                best_f2 = f2\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        return best_threshold, best_f2\n",
    "    \n",
    "    def compute_class_weights(self, snapshots):\n",
    "        \"\"\"Compute class weights for focal loss\"\"\"\n",
    "        all_labels = []\n",
    "        for snap in snapshots:\n",
    "            all_labels.extend(snap.y.cpu().numpy())\n",
    "        \n",
    "        all_labels = np.array(all_labels)\n",
    "        pos_weight = len(all_labels) / (2 * np.sum(all_labels))\n",
    "        return torch.tensor(pos_weight, dtype=torch.float).to(self.device)\n",
    "    \n",
    "    def train_model(self, snapshots, global_num_nodes):\n",
    "        \"\"\"Enhanced training with F2 optimization\"\"\"\n",
    "        \n",
    "        # Split data\n",
    "        train_size = int(len(snapshots) * (1 - self.config.VALIDATION_SPLIT - self.config.TEST_SPLIT))\n",
    "        val_size = int(len(snapshots) * self.config.VALIDATION_SPLIT)\n",
    "        \n",
    "        train_snaps = snapshots[:train_size]\n",
    "        val_snaps = snapshots[train_size:train_size + val_size]\n",
    "        test_snaps = snapshots[train_size + val_size:]\n",
    "        \n",
    "        print(f\"Data split - Train: {len(train_snaps)}, Val: {len(val_snaps)}, Test: {len(test_snaps)}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        model = TemporalEdgeClassifier(\n",
    "            self.config.NODE_DIM, \n",
    "            self.config.EDGE_DIM, \n",
    "            self.config.HIDDEN_DIM,\n",
    "            self.config.DROPOUT_RATE\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Compute class weights for focal loss\n",
    "        pos_weight = self.compute_class_weights(train_snaps)\n",
    "        criterion = FocalLoss(alpha=self.config.FOCAL_LOSS_ALPHA, gamma=self.config.FOCAL_LOSS_GAMMA)\n",
    "        \n",
    "        # Optimizer with different learning rates for different components\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': model.rnn.parameters(), 'lr': self.config.LEARNING_RATE * 0.5},\n",
    "            {'params': model.gnn1.parameters(), 'lr': self.config.LEARNING_RATE},\n",
    "            {'params': model.gnn2.parameters(), 'lr': self.config.LEARNING_RATE},\n",
    "            {'params': model.gnn3.parameters(), 'lr': self.config.LEARNING_RATE},\n",
    "            {'params': model.classifier.parameters(), 'lr': self.config.LEARNING_RATE * 1.5}\n",
    "        ], weight_decay=self.config.WEIGHT_DECAY)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=self.config.SCHEDULER_FACTOR, \n",
    "            patience=self.config.SCHEDULER_PATIENCE, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        best_f2_score = 0\n",
    "        patience_counter = 0\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "        f2_history = []\n",
    "        \n",
    "        for epoch in range(self.config.EPOCHS):\n",
    "            # Chunk size\n",
    "            k_steps = self.config.CHUNK_SIZE\n",
    "            \n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            # Initialize hidden state at the start of each epoch\n",
    "            h = torch.zeros(global_num_nodes, self.config.HIDDEN_DIM).to(self.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            chunk_loss = 0.0\n",
    "\n",
    "            if self.mem_profile:\n",
    "                print(f\"=== EPOCH {epoch} START ===\")\n",
    "                epoch_start_mem = detailed_memory_profile()\n",
    "                print(f\"Epoch Mem Allocated: {epoch_start_mem[0]:.3f} GB, Cached: {epoch_start_mem[0]:.3f} GB\")\n",
    "\n",
    "            for i, snap in enumerate(train_snaps):\n",
    "                snap = snap.to(self.device)\n",
    "                out, h = model(snap, h)      # Forward pass\n",
    "                loss = criterion(out.squeeze(), snap.y)  # Loss computation\n",
    "                chunk_loss += loss\n",
    "\n",
    "                # Backpropagation every k_steps\n",
    "                if (i + 1) % k_steps == 0 or (i + 1) == len(train_snaps):\n",
    "                    chunk_loss /= k_steps\n",
    "                    # Backward pass\n",
    "                    chunk_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Gradient clipping\n",
    "                    optimizer.step()\n",
    "                    # Detach h\n",
    "                    h = h.detach()\n",
    "                    # Reset for next chunk\n",
    "                    optimizer.zero_grad()\n",
    "                    chunk_loss = 0.0\n",
    "\n",
    "                # Total epoch loss\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_snaps)\n",
    "            train_loss_history.append(avg_train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_probs_list, val_labels_list = [], []\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                h = torch.zeros(global_num_nodes, self.config.HIDDEN_DIM).to(self.device)\n",
    "                for snap in val_snaps:\n",
    "                    snap = snap.to(self.device)\n",
    "                    out, h = model(snap, h)\n",
    "                    loss = criterion(out.squeeze(), snap.y)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    preds = torch.sigmoid(out).squeeze()\n",
    "                    val_probs_list.append(preds.cpu())\n",
    "                    val_labels_list.append(snap.y.cpu())\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_snaps)\n",
    "            val_loss_history.append(avg_val_loss)\n",
    "            \n",
    "            # Calculate F2 score with optimal threshold\n",
    "            val_probs = torch.cat(val_probs_list).numpy()\n",
    "            val_labels = torch.cat(val_labels_list).numpy()\n",
    "            \n",
    "            optimal_threshold, f2_score = self.find_optimal_threshold(val_probs, val_labels)\n",
    "            f2_history.append(f2_score)\n",
    "            recall = recall_score(val_labels, (val_probs >= optimal_threshold).astype(int), zero_division=0)\n",
    "            \n",
    "            # Update scheduler with F2 score\n",
    "            # scheduler.step(f2_score)\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Early stopping based on F2 score\n",
    "            if f2_score > best_f2_score:\n",
    "                best_f2_score = f2_score\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                # torch.save(model.state_dict(), './outputs/best_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # if (epoch + 1) % 10 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch+1}: Train Loss(x1e3): {1000*avg_train_loss:.4f}, Val Loss(x1e3): {1000*avg_val_loss:.4f}, \"\n",
    "                        f\"F2: {f2_score:.4f}, Threshold: {optimal_threshold:.3f}, Recall: {recall:.4f}, \"\n",
    "                        f\"LR: {current_lr:.6f}\")\n",
    "\n",
    "            if patience_counter >= self.config.PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "        \n",
    "        # Load best model and evaluate\n",
    "        # model.load_state_dict(torch.load('./outputs/best_model.pth'))\n",
    "        \n",
    "        # Final evaluation\n",
    "        results = self._evaluate_model(model, train_snaps, val_snaps, test_snaps, global_num_nodes)\n",
    "        results.update({\n",
    "            'train_loss_history': train_loss_history,\n",
    "            'val_loss_history': val_loss_history,\n",
    "            'f2_history': f2_history,\n",
    "            'model': model\n",
    "        })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _evaluate_model(self, model, train_snaps, val_snaps, test_snaps, global_num_nodes):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        model.eval()\n",
    "        results = {}\n",
    "        \n",
    "        for split_name, snaps in [('val', val_snaps), ('test', test_snaps)]:\n",
    "            probs_list, labels_list = [], []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                h = torch.zeros(global_num_nodes, self.config.HIDDEN_DIM).to(self.device)\n",
    "                for snap in snaps:\n",
    "                    snap = snap.to(self.device)\n",
    "                    out, h = model(snap, h)\n",
    "                    preds = torch.sigmoid(out).squeeze().cpu().numpy()\n",
    "                    probs_list.extend(preds)\n",
    "                    labels_list.extend(snap.y.cpu().numpy())\n",
    "            \n",
    "            probs = np.array(probs_list)\n",
    "            labels = np.array(labels_list)\n",
    "            \n",
    "            # Find optimal threshold\n",
    "            optimal_threshold, best_f2 = self.find_optimal_threshold(probs, labels)\n",
    "            binary_preds = (probs >= optimal_threshold).astype(int)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            precision = precision_score(labels, binary_preds, zero_division=0)\n",
    "            recall = recall_score(labels, binary_preds, zero_division=0)\n",
    "            f1 = f1_score(labels, binary_preds, zero_division=0)\n",
    "            roc_auc = roc_auc_score(labels, probs)\n",
    "            pr_auc = average_precision_score(labels, probs)\n",
    "            \n",
    "            results[f'{split_name}_probs'] = probs\n",
    "            results[f'{split_name}_labels'] = labels\n",
    "            results[f'{split_name}_threshold'] = optimal_threshold\n",
    "            results[f'{split_name}_precision'] = precision\n",
    "            results[f'{split_name}_recall'] = recall\n",
    "            results[f'{split_name}_f1'] = f1\n",
    "            results[f'{split_name}_f2'] = best_f2\n",
    "            results[f'{split_name}_roc_auc'] = roc_auc\n",
    "            results[f'{split_name}_pr_auc'] = pr_auc\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2124555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire dataset\n",
    "df = pd.read_csv(DATAPATH)\n",
    "\n",
    "# Filter by data range\n",
    "# df = df[df['Date'] < '2023-08-18']\n",
    "# df = df.head(300000).copy()\n",
    "\n",
    "# run feature engg.ipynb to get the account_stats_7D.csv\n",
    "account_stats = pd.read_csv('../account_stats_7D.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9178de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Loaded 9504852 transactions\n",
      "Suspicious transactions: 9873 (0.104%)\n",
      "Engineering enhanced features...\n"
     ]
    }
   ],
   "source": [
    "graph_processor = TemporalGraphDataProcessor()\n",
    "df = graph_processor.load_and_preprocess(df)\n",
    "df = graph_processor.engineer_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8bfb9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# For each categorical column\n",
    "# categorical_cols = ['Payment_currency', 'Received_currency', 'Sender_bank_location', \n",
    "#                    'Receiver_bank_location', 'Payment_type']\n",
    "categorical_cols = ['Payment_type']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "    \n",
    "# Drop original object columns\n",
    "df = df.drop(categorical_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43c740f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process accont_stats\n",
    "columns = ['med_sent_amt', 'std_sent_amt', 'med_recv_amt', 'std_recv_amt', \n",
    "           'max_sent_txn_amt', 'max_recv_txn_amt', 'total_txns_amt']\n",
    "\n",
    "for col in columns:\n",
    "    account_stats['log_' + col] = np.log1p(account_stats[col]).astype('float32')\n",
    "\n",
    "account_stats = account_stats.drop(columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "094d1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data types to optimize memory\n",
    "account_stats = account_stats.astype({\n",
    "    'sent_txns_count': 'int32',\n",
    "    'recv_txns_count': 'int32',\n",
    "    'fan_out': 'int32',\n",
    "    'fan_in': 'int32',\n",
    "    'max_sent_txn_count': 'int32',\n",
    "    'max_recv_txn_count': 'int32',\n",
    "    'sent_recv_ratio': 'float32',\n",
    "    'fanout_fanin_ratio': 'float32'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79bf8e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporal graph snapshots...\n",
      "Processing time range: 2022-10-07 to 2023-08-23\n",
      "Processing window: 2022-10-07 to 2022-10-14\n",
      "Processing window: 2022-10-14 to 2022-10-21\n",
      "Processing window: 2022-10-21 to 2022-10-28\n",
      "Processing window: 2022-10-28 to 2022-11-04\n",
      "Processing window: 2022-11-04 to 2022-11-11\n",
      "Processing window: 2022-11-11 to 2022-11-18\n",
      "Processing window: 2022-11-18 to 2022-11-25\n",
      "Processing window: 2022-11-25 to 2022-12-02\n",
      "Processing window: 2022-12-02 to 2022-12-09\n",
      "Processing window: 2022-12-09 to 2022-12-16\n",
      "Processing window: 2022-12-16 to 2022-12-23\n",
      "Processing window: 2022-12-23 to 2022-12-30\n",
      "Processing window: 2022-12-30 to 2023-01-06\n",
      "Processing window: 2023-01-06 to 2023-01-13\n",
      "Processing window: 2023-01-13 to 2023-01-20\n",
      "Processing window: 2023-01-20 to 2023-01-27\n",
      "Processing window: 2023-01-27 to 2023-02-03\n",
      "Processing window: 2023-02-03 to 2023-02-10\n",
      "Processing window: 2023-02-10 to 2023-02-17\n",
      "Processing window: 2023-02-17 to 2023-02-24\n",
      "Processing window: 2023-02-24 to 2023-03-03\n",
      "Processing window: 2023-03-03 to 2023-03-10\n",
      "Processing window: 2023-03-10 to 2023-03-17\n",
      "Processing window: 2023-03-17 to 2023-03-24\n",
      "Processing window: 2023-03-24 to 2023-03-31\n",
      "Processing window: 2023-03-31 to 2023-04-07\n",
      "Processing window: 2023-04-07 to 2023-04-14\n",
      "Processing window: 2023-04-14 to 2023-04-21\n",
      "Processing window: 2023-04-21 to 2023-04-28\n",
      "Processing window: 2023-04-28 to 2023-05-05\n",
      "Processing window: 2023-05-05 to 2023-05-12\n",
      "Processing window: 2023-05-12 to 2023-05-19\n",
      "Processing window: 2023-05-19 to 2023-05-26\n",
      "Processing window: 2023-05-26 to 2023-06-02\n",
      "Processing window: 2023-06-02 to 2023-06-09\n",
      "Processing window: 2023-06-09 to 2023-06-16\n",
      "Processing window: 2023-06-16 to 2023-06-23\n",
      "Processing window: 2023-06-23 to 2023-06-30\n",
      "Processing window: 2023-06-30 to 2023-07-07\n",
      "Processing window: 2023-07-07 to 2023-07-14\n",
      "Processing window: 2023-07-14 to 2023-07-21\n",
      "Processing window: 2023-07-21 to 2023-07-28\n",
      "Processing window: 2023-07-28 to 2023-08-04\n",
      "Processing window: 2023-08-04 to 2023-08-11\n",
      "Processing window: 2023-08-11 to 2023-08-18\n",
      "Processing window: 2023-08-18 to 2023-08-25\n",
      "Created 46 temporal snapshots\n"
     ]
    }
   ],
   "source": [
    "snapshots, global_num_nodes = graph_processor.create_temporal_snapshots(df, account_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a083f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.00GB, Cached: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89fc6783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Data split - Train: 32, Val: 7, Test: 7\n",
      "Epoch 1: Train Loss(x1e3): 21.8818, Val Loss(x1e3): 1.5905, F2: 0.0086, Threshold: 0.250, Recall: 0.0480, LR: 0.000250\n",
      "Epoch 2: Train Loss(x1e3): 0.9084, Val Loss(x1e3): 0.9732, F2: 0.0088, Threshold: 0.050, Recall: 0.1063, LR: 0.000250\n",
      "Epoch 3: Train Loss(x1e3): 1.1293, Val Loss(x1e3): 1.1678, F2: 0.0070, Threshold: 0.050, Recall: 0.0137, LR: 0.000250\n",
      "Epoch 4: Train Loss(x1e3): 1.1385, Val Loss(x1e3): 1.0252, F2: 0.0084, Threshold: 0.050, Recall: 0.0425, LR: 0.000250\n",
      "Epoch 5: Train Loss(x1e3): 0.9672, Val Loss(x1e3): 0.8460, F2: 0.0094, Threshold: 0.050, Recall: 0.1804, LR: 0.000250\n",
      "Epoch 6: Train Loss(x1e3): 0.8142, Val Loss(x1e3): 0.7222, F2: 0.0088, Threshold: 0.050, Recall: 0.4979, LR: 0.000250\n",
      "Epoch 7: Train Loss(x1e3): 0.7451, Val Loss(x1e3): 0.6846, F2: 0.0092, Threshold: 0.100, Recall: 0.2051, LR: 0.000250\n",
      "Epoch 8: Train Loss(x1e3): 0.7366, Val Loss(x1e3): 0.6765, F2: 0.0094, Threshold: 0.100, Recall: 0.2298, LR: 0.000250\n",
      "Epoch 9: Train Loss(x1e3): 0.7146, Val Loss(x1e3): 0.6692, F2: 0.0097, Threshold: 0.100, Recall: 0.1907, LR: 0.000250\n",
      "Epoch 10: Train Loss(x1e3): 0.7061, Val Loss(x1e3): 0.6641, F2: 0.0098, Threshold: 0.100, Recall: 0.1674, LR: 0.000250\n",
      "Epoch 11: Train Loss(x1e3): 0.6997, Val Loss(x1e3): 0.6555, F2: 0.0100, Threshold: 0.100, Recall: 0.1763, LR: 0.000250\n",
      "Epoch 12: Train Loss(x1e3): 0.6920, Val Loss(x1e3): 0.6453, F2: 0.0105, Threshold: 0.100, Recall: 0.1989, LR: 0.000250\n",
      "Epoch 13: Train Loss(x1e3): 0.6817, Val Loss(x1e3): 0.6345, F2: 0.0116, Threshold: 0.100, Recall: 0.2291, LR: 0.000250\n",
      "Epoch 14: Train Loss(x1e3): 0.6733, Val Loss(x1e3): 0.6246, F2: 0.0129, Threshold: 0.100, Recall: 0.2586, LR: 0.000250\n",
      "Epoch 15: Train Loss(x1e3): 0.6621, Val Loss(x1e3): 0.6151, F2: 0.0142, Threshold: 0.100, Recall: 0.2867, LR: 0.000250\n",
      "Epoch 16: Train Loss(x1e3): 0.6532, Val Loss(x1e3): 0.6065, F2: 0.0162, Threshold: 0.100, Recall: 0.3162, LR: 0.000250\n",
      "Epoch 17: Train Loss(x1e3): 0.6472, Val Loss(x1e3): 0.5978, F2: 0.0181, Threshold: 0.100, Recall: 0.3560, LR: 0.000250\n",
      "Epoch 18: Train Loss(x1e3): 0.6381, Val Loss(x1e3): 0.5889, F2: 0.0202, Threshold: 0.100, Recall: 0.3923, LR: 0.000250\n",
      "Epoch 19: Train Loss(x1e3): 0.6288, Val Loss(x1e3): 0.5795, F2: 0.0231, Threshold: 0.100, Recall: 0.4486, LR: 0.000250\n",
      "Epoch 20: Train Loss(x1e3): 0.6210, Val Loss(x1e3): 0.5695, F2: 0.0259, Threshold: 0.100, Recall: 0.5041, LR: 0.000250\n",
      "Epoch 21: Train Loss(x1e3): 0.6143, Val Loss(x1e3): 0.5588, F2: 0.0293, Threshold: 0.100, Recall: 0.5741, LR: 0.000250\n",
      "Epoch 22: Train Loss(x1e3): 0.5999, Val Loss(x1e3): 0.5466, F2: 0.0327, Threshold: 0.100, Recall: 0.6262, LR: 0.000250\n",
      "Epoch 23: Train Loss(x1e3): 0.5842, Val Loss(x1e3): 0.5333, F2: 0.0357, Threshold: 0.100, Recall: 0.6763, LR: 0.000250\n",
      "Epoch 24: Train Loss(x1e3): 0.5724, Val Loss(x1e3): 0.5186, F2: 0.0387, Threshold: 0.150, Recall: 0.2270, LR: 0.000250\n",
      "Epoch 25: Train Loss(x1e3): 0.5608, Val Loss(x1e3): 0.5029, F2: 0.0476, Threshold: 0.150, Recall: 0.2840, LR: 0.000250\n",
      "Epoch 26: Train Loss(x1e3): 0.5496, Val Loss(x1e3): 0.4870, F2: 0.0617, Threshold: 0.150, Recall: 0.3628, LR: 0.000250\n",
      "Epoch 27: Train Loss(x1e3): 0.5353, Val Loss(x1e3): 0.4695, F2: 0.0797, Threshold: 0.150, Recall: 0.4431, LR: 0.000250\n",
      "Epoch 28: Train Loss(x1e3): 0.5193, Val Loss(x1e3): 0.4518, F2: 0.1124, Threshold: 0.200, Recall: 0.1358, LR: 0.000250\n",
      "Epoch 29: Train Loss(x1e3): 0.5067, Val Loss(x1e3): 0.4352, F2: 0.1573, Threshold: 0.200, Recall: 0.2181, LR: 0.000250\n",
      "Epoch 30: Train Loss(x1e3): 0.4905, Val Loss(x1e3): 0.4173, F2: 0.2042, Threshold: 0.200, Recall: 0.3107, LR: 0.000250\n",
      "Epoch 31: Train Loss(x1e3): 0.4793, Val Loss(x1e3): 0.4009, F2: 0.2406, Threshold: 0.200, Recall: 0.3971, LR: 0.000250\n",
      "Epoch 32: Train Loss(x1e3): 0.4642, Val Loss(x1e3): 0.3865, F2: 0.2664, Threshold: 0.200, Recall: 0.4945, LR: 0.000250\n",
      "Epoch 33: Train Loss(x1e3): 0.4565, Val Loss(x1e3): 0.3718, F2: 0.2949, Threshold: 0.200, Recall: 0.5631, LR: 0.000250\n",
      "Epoch 34: Train Loss(x1e3): 0.4443, Val Loss(x1e3): 0.3594, F2: 0.3003, Threshold: 0.200, Recall: 0.6193, LR: 0.000250\n",
      "Epoch 35: Train Loss(x1e3): 0.4348, Val Loss(x1e3): 0.3502, F2: 0.3229, Threshold: 0.250, Recall: 0.3916, LR: 0.000250\n",
      "Epoch 36: Train Loss(x1e3): 0.4227, Val Loss(x1e3): 0.3393, F2: 0.3645, Threshold: 0.250, Recall: 0.4712, LR: 0.000250\n",
      "Epoch 37: Train Loss(x1e3): 0.4058, Val Loss(x1e3): 0.3285, F2: 0.3881, Threshold: 0.250, Recall: 0.5199, LR: 0.000250\n",
      "Epoch 38: Train Loss(x1e3): 0.3951, Val Loss(x1e3): 0.3187, F2: 0.4016, Threshold: 0.250, Recall: 0.5823, LR: 0.000250\n",
      "Epoch 39: Train Loss(x1e3): 0.3844, Val Loss(x1e3): 0.3165, F2: 0.3909, Threshold: 0.250, Recall: 0.6406, LR: 0.000250\n",
      "Epoch 40: Train Loss(x1e3): 0.3746, Val Loss(x1e3): 0.3153, F2: 0.4296, Threshold: 0.300, Recall: 0.4787, LR: 0.000250\n",
      "Epoch 41: Train Loss(x1e3): 0.3660, Val Loss(x1e3): 0.3040, F2: 0.4578, Threshold: 0.300, Recall: 0.5062, LR: 0.000250\n",
      "Epoch 42: Train Loss(x1e3): 0.3498, Val Loss(x1e3): 0.2920, F2: 0.4819, Threshold: 0.300, Recall: 0.5446, LR: 0.000250\n",
      "Epoch 43: Train Loss(x1e3): 0.3363, Val Loss(x1e3): 0.2945, F2: 0.5038, Threshold: 0.300, Recall: 0.6337, LR: 0.000250\n",
      "Epoch 44: Train Loss(x1e3): 0.3334, Val Loss(x1e3): 0.2810, F2: 0.5184, Threshold: 0.300, Recall: 0.6536, LR: 0.000250\n",
      "Epoch 45: Train Loss(x1e3): 0.3167, Val Loss(x1e3): 0.2580, F2: 0.5541, Threshold: 0.300, Recall: 0.6502, LR: 0.000250\n",
      "Epoch 46: Train Loss(x1e3): 0.3046, Val Loss(x1e3): 0.2664, F2: 0.5548, Threshold: 0.350, Recall: 0.5857, LR: 0.000250\n",
      "Epoch 47: Train Loss(x1e3): 0.3076, Val Loss(x1e3): 0.2696, F2: 0.5712, Threshold: 0.350, Recall: 0.6447, LR: 0.000250\n",
      "Epoch 48: Train Loss(x1e3): 0.2971, Val Loss(x1e3): 0.2324, F2: 0.5943, Threshold: 0.350, Recall: 0.6056, LR: 0.000250\n",
      "Epoch 49: Train Loss(x1e3): 0.2731, Val Loss(x1e3): 0.2296, F2: 0.6011, Threshold: 0.350, Recall: 0.6296, LR: 0.000250\n",
      "Epoch 50: Train Loss(x1e3): 0.2726, Val Loss(x1e3): 0.2787, F2: 0.6036, Threshold: 0.400, Recall: 0.6488, LR: 0.000250\n",
      "Epoch 51: Train Loss(x1e3): 0.2996, Val Loss(x1e3): 0.2210, F2: 0.6155, Threshold: 0.350, Recall: 0.6331, LR: 0.000250\n",
      "Epoch 52: Train Loss(x1e3): 0.2658, Val Loss(x1e3): 0.2122, F2: 0.6280, Threshold: 0.300, Recall: 0.6529, LR: 0.000250\n",
      "Epoch 53: Train Loss(x1e3): 0.2603, Val Loss(x1e3): 0.2655, F2: 0.6177, Threshold: 0.400, Recall: 0.6879, LR: 0.000250\n",
      "Epoch 54: Train Loss(x1e3): 0.2700, Val Loss(x1e3): 0.2061, F2: 0.6368, Threshold: 0.350, Recall: 0.6632, LR: 0.000250\n",
      "Epoch 55: Train Loss(x1e3): 0.2414, Val Loss(x1e3): 0.1941, F2: 0.6576, Threshold: 0.350, Recall: 0.6770, LR: 0.000250\n",
      "Epoch 56: Train Loss(x1e3): 0.2354, Val Loss(x1e3): 0.2310, F2: 0.6642, Threshold: 0.450, Recall: 0.6660, LR: 0.000250\n",
      "Epoch 57: Train Loss(x1e3): 0.2346, Val Loss(x1e3): 0.1934, F2: 0.6729, Threshold: 0.400, Recall: 0.6804, LR: 0.000250\n",
      "Epoch 58: Train Loss(x1e3): 0.2155, Val Loss(x1e3): 0.1812, F2: 0.6925, Threshold: 0.400, Recall: 0.7003, LR: 0.000250\n",
      "Epoch 59: Train Loss(x1e3): 0.2099, Val Loss(x1e3): 0.2060, F2: 0.6954, Threshold: 0.450, Recall: 0.7016, LR: 0.000250\n",
      "Epoch 60: Train Loss(x1e3): 0.2158, Val Loss(x1e3): 0.1711, F2: 0.7142, Threshold: 0.400, Recall: 0.7257, LR: 0.000250\n",
      "Epoch 61: Train Loss(x1e3): 0.1961, Val Loss(x1e3): 0.1670, F2: 0.7166, Threshold: 0.400, Recall: 0.7270, LR: 0.000250\n",
      "Epoch 62: Train Loss(x1e3): 0.1937, Val Loss(x1e3): 0.1756, F2: 0.7270, Threshold: 0.450, Recall: 0.7305, LR: 0.000250\n",
      "Epoch 63: Train Loss(x1e3): 0.1938, Val Loss(x1e3): 0.1611, F2: 0.7389, Threshold: 0.400, Recall: 0.7647, LR: 0.000250\n",
      "Epoch 64: Train Loss(x1e3): 0.1790, Val Loss(x1e3): 0.1507, F2: 0.7473, Threshold: 0.400, Recall: 0.7627, LR: 0.000250\n",
      "Epoch 65: Train Loss(x1e3): 0.1764, Val Loss(x1e3): 0.1516, F2: 0.7477, Threshold: 0.450, Recall: 0.7435, LR: 0.000250\n",
      "Epoch 66: Train Loss(x1e3): 0.1692, Val Loss(x1e3): 0.1415, F2: 0.7545, Threshold: 0.400, Recall: 0.7716, LR: 0.000250\n",
      "Epoch 67: Train Loss(x1e3): 0.1655, Val Loss(x1e3): 0.1392, F2: 0.7649, Threshold: 0.400, Recall: 0.7894, LR: 0.000250\n",
      "Epoch 68: Train Loss(x1e3): 0.1597, Val Loss(x1e3): 0.1369, F2: 0.7667, Threshold: 0.400, Recall: 0.7908, LR: 0.000250\n",
      "Epoch 69: Train Loss(x1e3): 0.1540, Val Loss(x1e3): 0.1337, F2: 0.7722, Threshold: 0.400, Recall: 0.7922, LR: 0.000250\n",
      "Epoch 70: Train Loss(x1e3): 0.1507, Val Loss(x1e3): 0.1330, F2: 0.7742, Threshold: 0.400, Recall: 0.7970, LR: 0.000250\n",
      "Epoch 71: Train Loss(x1e3): 0.1406, Val Loss(x1e3): 0.1329, F2: 0.7752, Threshold: 0.450, Recall: 0.7778, LR: 0.000250\n",
      "Epoch 72: Train Loss(x1e3): 0.1481, Val Loss(x1e3): 0.1290, F2: 0.7808, Threshold: 0.400, Recall: 0.7990, LR: 0.000250\n",
      "Epoch 73: Train Loss(x1e3): 0.1410, Val Loss(x1e3): 0.1243, F2: 0.7857, Threshold: 0.400, Recall: 0.7942, LR: 0.000250\n",
      "Epoch 74: Train Loss(x1e3): 0.1375, Val Loss(x1e3): 0.1257, F2: 0.7857, Threshold: 0.400, Recall: 0.8059, LR: 0.000250\n",
      "Epoch 75: Train Loss(x1e3): 0.1406, Val Loss(x1e3): 0.1241, F2: 0.7877, Threshold: 0.400, Recall: 0.8004, LR: 0.000250\n",
      "Epoch 76: Train Loss(x1e3): 0.1329, Val Loss(x1e3): 0.1225, F2: 0.7876, Threshold: 0.400, Recall: 0.7936, LR: 0.000250\n",
      "Epoch 77: Train Loss(x1e3): 0.1318, Val Loss(x1e3): 0.1259, F2: 0.7886, Threshold: 0.400, Recall: 0.8080, LR: 0.000250\n",
      "Epoch 78: Train Loss(x1e3): 0.1299, Val Loss(x1e3): 0.1259, F2: 0.7895, Threshold: 0.400, Recall: 0.8045, LR: 0.000250\n",
      "Epoch 79: Train Loss(x1e3): 0.1244, Val Loss(x1e3): 0.1201, F2: 0.7957, Threshold: 0.400, Recall: 0.8038, LR: 0.000250\n",
      "Epoch 80: Train Loss(x1e3): 0.1254, Val Loss(x1e3): 0.1282, F2: 0.7907, Threshold: 0.450, Recall: 0.7936, LR: 0.000250\n",
      "Epoch 81: Train Loss(x1e3): 0.1258, Val Loss(x1e3): 0.1186, F2: 0.7959, Threshold: 0.400, Recall: 0.7977, LR: 0.000250\n",
      "Epoch 82: Train Loss(x1e3): 0.1233, Val Loss(x1e3): 0.1266, F2: 0.7941, Threshold: 0.450, Recall: 0.8004, LR: 0.000250\n",
      "Epoch 83: Train Loss(x1e3): 0.1237, Val Loss(x1e3): 0.1176, F2: 0.8007, Threshold: 0.400, Recall: 0.8038, LR: 0.000250\n",
      "Epoch 84: Train Loss(x1e3): 0.1184, Val Loss(x1e3): 0.1211, F2: 0.7971, Threshold: 0.400, Recall: 0.8114, LR: 0.000250\n",
      "Epoch 85: Train Loss(x1e3): 0.1166, Val Loss(x1e3): 0.1174, F2: 0.8015, Threshold: 0.400, Recall: 0.8114, LR: 0.000250\n",
      "Epoch 86: Train Loss(x1e3): 0.1239, Val Loss(x1e3): 0.1197, F2: 0.8002, Threshold: 0.400, Recall: 0.8196, LR: 0.000250\n",
      "Epoch 87: Train Loss(x1e3): 0.1190, Val Loss(x1e3): 0.1179, F2: 0.8015, Threshold: 0.400, Recall: 0.8080, LR: 0.000250\n",
      "Epoch 88: Train Loss(x1e3): 0.1131, Val Loss(x1e3): 0.1159, F2: 0.8040, Threshold: 0.400, Recall: 0.8086, LR: 0.000250\n",
      "Epoch 89: Train Loss(x1e3): 0.1173, Val Loss(x1e3): 0.1225, F2: 0.8073, Threshold: 0.450, Recall: 0.8121, LR: 0.000250\n",
      "Epoch 90: Train Loss(x1e3): 0.1174, Val Loss(x1e3): 0.1152, F2: 0.8045, Threshold: 0.350, Recall: 0.8189, LR: 0.000250\n",
      "Epoch 91: Train Loss(x1e3): 0.1115, Val Loss(x1e3): 0.1135, F2: 0.8060, Threshold: 0.350, Recall: 0.8251, LR: 0.000250\n",
      "Epoch 92: Train Loss(x1e3): 0.1171, Val Loss(x1e3): 0.1271, F2: 0.8021, Threshold: 0.450, Recall: 0.8141, LR: 0.000250\n",
      "Epoch 93: Train Loss(x1e3): 0.1218, Val Loss(x1e3): 0.1196, F2: 0.7971, Threshold: 0.350, Recall: 0.8080, LR: 0.000250\n",
      "Epoch 94: Train Loss(x1e3): 0.1136, Val Loss(x1e3): 0.1182, F2: 0.7997, Threshold: 0.300, Recall: 0.8244, LR: 0.000250\n",
      "Epoch 95: Train Loss(x1e3): 0.1148, Val Loss(x1e3): 0.1242, F2: 0.8108, Threshold: 0.450, Recall: 0.8230, LR: 0.000125\n",
      "Epoch 96: Train Loss(x1e3): 0.1127, Val Loss(x1e3): 0.1171, F2: 0.8027, Threshold: 0.350, Recall: 0.8107, LR: 0.000125\n",
      "Epoch 97: Train Loss(x1e3): 0.1073, Val Loss(x1e3): 0.1142, F2: 0.8153, Threshold: 0.400, Recall: 0.8285, LR: 0.000125\n",
      "Epoch 98: Train Loss(x1e3): 0.1067, Val Loss(x1e3): 0.1133, F2: 0.8079, Threshold: 0.400, Recall: 0.8093, LR: 0.000125\n",
      "Epoch 99: Train Loss(x1e3): 0.1094, Val Loss(x1e3): 0.1137, F2: 0.8087, Threshold: 0.350, Recall: 0.8395, LR: 0.000125\n",
      "Epoch 100: Train Loss(x1e3): 0.1045, Val Loss(x1e3): 0.1126, F2: 0.8096, Threshold: 0.400, Recall: 0.8148, LR: 0.000125\n",
      "Epoch 101: Train Loss(x1e3): 0.1057, Val Loss(x1e3): 0.1128, F2: 0.8102, Threshold: 0.400, Recall: 0.8182, LR: 0.000125\n",
      "Epoch 102: Train Loss(x1e3): 0.1055, Val Loss(x1e3): 0.1112, F2: 0.8141, Threshold: 0.400, Recall: 0.8217, LR: 0.000125\n",
      "Epoch 103: Train Loss(x1e3): 0.1040, Val Loss(x1e3): 0.1121, F2: 0.8113, Threshold: 0.400, Recall: 0.8148, LR: 0.000125\n",
      "Epoch 104: Train Loss(x1e3): 0.1028, Val Loss(x1e3): 0.1117, F2: 0.8127, Threshold: 0.400, Recall: 0.8244, LR: 0.000125\n",
      "Epoch 105: Train Loss(x1e3): 0.1011, Val Loss(x1e3): 0.1122, F2: 0.8095, Threshold: 0.400, Recall: 0.8114, LR: 0.000125\n",
      "Epoch 106: Train Loss(x1e3): 0.1013, Val Loss(x1e3): 0.1117, F2: 0.8145, Threshold: 0.450, Recall: 0.8114, LR: 0.000063\n",
      "Epoch 107: Train Loss(x1e3): 0.1012, Val Loss(x1e3): 0.1100, F2: 0.8149, Threshold: 0.400, Recall: 0.8196, LR: 0.000063\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(config=Config(), mem_profile=False)\n",
    "results = trainer.train_model(snapshots, global_num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb0d7038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_probs': array([0.09558561, 0.06335507, 0.01970434, ..., 0.00411201, 0.00310848,\n",
       "        0.00374726], shape=(1458820,), dtype=float32),\n",
       " 'val_labels': array([0., 0., 0., ..., 0., 0., 0.], shape=(1458820,), dtype=float32),\n",
       " 'val_threshold': np.float64(0.4),\n",
       " 'val_precision': 0.7966666666666666,\n",
       " 'val_recall': 0.8196159122085048,\n",
       " 'val_f1': 0.8079783637592968,\n",
       " 'val_f2': 0.8149208947081288,\n",
       " 'val_roc_auc': 0.9960667031860406,\n",
       " 'val_pr_auc': 0.8505354058302449,\n",
       " 'test_probs': array([0.00077067, 0.00448026, 0.00494907, ..., 0.0033213 , 0.15114045,\n",
       "        0.01583933], shape=(1384809,), dtype=float32),\n",
       " 'test_labels': array([0., 0., 0., ..., 0., 0., 0.], shape=(1384809,), dtype=float32),\n",
       " 'test_threshold': np.float64(0.4),\n",
       " 'test_precision': 0.8179611650485437,\n",
       " 'test_recall': 0.8214503351614869,\n",
       " 'test_f1': 0.8197020370933414,\n",
       " 'test_f2': 0.8207501217730151,\n",
       " 'test_roc_auc': 0.9947576201755226,\n",
       " 'test_pr_auc': 0.8561839046265236,\n",
       " 'train_loss_history': [0.021881783584831282,\n",
       "  0.0009083815984922694,\n",
       "  0.0011292642175249057,\n",
       "  0.001138526844442822,\n",
       "  0.0009671906409494113,\n",
       "  0.0008142410151776858,\n",
       "  0.0007450548073393293,\n",
       "  0.0007365853007286205,\n",
       "  0.0007146111520341947,\n",
       "  0.000706081008502224,\n",
       "  0.0006996583897489472,\n",
       "  0.0006920380037627183,\n",
       "  0.0006817439407313941,\n",
       "  0.000673289729093085,\n",
       "  0.0006620925132665434,\n",
       "  0.0006531980770887458,\n",
       "  0.0006472016675616032,\n",
       "  0.0006381369421433192,\n",
       "  0.0006287893620537943,\n",
       "  0.000620999077000306,\n",
       "  0.0006143457212601788,\n",
       "  0.0005998670085318736,\n",
       "  0.0005841896563651972,\n",
       "  0.0005723688063881127,\n",
       "  0.0005607938537650625,\n",
       "  0.0005496458779816749,\n",
       "  0.0005353180049496586,\n",
       "  0.0005192929947952507,\n",
       "  0.0005067257616246934,\n",
       "  0.0004905305240754387,\n",
       "  0.0004792500149051193,\n",
       "  0.000464242748421384,\n",
       "  0.00045652789958694484,\n",
       "  0.00044432087088353,\n",
       "  0.00043484621073730523,\n",
       "  0.0004226751989335753,\n",
       "  0.00040578487823950127,\n",
       "  0.00039512431612820365,\n",
       "  0.0003843951153612579,\n",
       "  0.0003746105385289411,\n",
       "  0.0003660382726593525,\n",
       "  0.0003498452279018238,\n",
       "  0.00033630868392720004,\n",
       "  0.0003334020543661609,\n",
       "  0.0003167131735608564,\n",
       "  0.00030463703251371044,\n",
       "  0.00030755504394619493,\n",
       "  0.0002971431599689822,\n",
       "  0.0002731082968239207,\n",
       "  0.00027263907713859226,\n",
       "  0.0002996273769895197,\n",
       "  0.000265841797045141,\n",
       "  0.0002602630220280844,\n",
       "  0.000269983836915344,\n",
       "  0.00024139089828167926,\n",
       "  0.00023544098576167016,\n",
       "  0.00023455969130736776,\n",
       "  0.00021546771586145042,\n",
       "  0.0002098957093039644,\n",
       "  0.00021578498126473278,\n",
       "  0.00019606165710683854,\n",
       "  0.00019367659774616186,\n",
       "  0.00019380612229724647,\n",
       "  0.00017900436387208174,\n",
       "  0.00017643588353166706,\n",
       "  0.00016923966086324072,\n",
       "  0.00016554007220292988,\n",
       "  0.00015967142962836078,\n",
       "  0.00015403046995743352,\n",
       "  0.0001507408585439407,\n",
       "  0.0001406225826485752,\n",
       "  0.00014809514414082514,\n",
       "  0.00014099838244874263,\n",
       "  0.0001374796934214828,\n",
       "  0.00014059869272387004,\n",
       "  0.00013292801554598555,\n",
       "  0.00013181377289583907,\n",
       "  0.00012993818677387026,\n",
       "  0.00012435605685823248,\n",
       "  0.0001253648258625617,\n",
       "  0.0001258275801774289,\n",
       "  0.00012331859966252523,\n",
       "  0.00012374325456221413,\n",
       "  0.0001184466407266882,\n",
       "  0.00011659357164717221,\n",
       "  0.0001238524826021603,\n",
       "  0.00011903327686013654,\n",
       "  0.00011306182295811595,\n",
       "  0.00011725674607987457,\n",
       "  0.00011736882629520551,\n",
       "  0.00011152574802508752,\n",
       "  0.00011710679814314062,\n",
       "  0.0001217733693010814,\n",
       "  0.00011357198138739477,\n",
       "  0.00011481218564313167,\n",
       "  0.00011267652837432252,\n",
       "  0.00010730151939242205,\n",
       "  0.00010673059637156257,\n",
       "  0.00010944615166863514,\n",
       "  0.00010454010748617293,\n",
       "  0.00010565075854174211,\n",
       "  0.00010550419722221704,\n",
       "  0.00010404206568637164,\n",
       "  0.0001027907291017982,\n",
       "  0.00010111054120898189,\n",
       "  0.00010125313656317303,\n",
       "  0.00010120487218046037],\n",
       " 'val_loss_history': [0.0015904638151239073,\n",
       "  0.0009732424264906772,\n",
       "  0.0011677764123305678,\n",
       "  0.0010252101180542792,\n",
       "  0.0008459959727978068,\n",
       "  0.0007221784575709275,\n",
       "  0.0006846373351956052,\n",
       "  0.0006764661853334733,\n",
       "  0.0006691796505557639,\n",
       "  0.0006641261612198182,\n",
       "  0.0006554795836564153,\n",
       "  0.0006452587216959468,\n",
       "  0.0006344629842455366,\n",
       "  0.0006246318766248546,\n",
       "  0.0006150533569910165,\n",
       "  0.000606469768432102,\n",
       "  0.0005977909562976233,\n",
       "  0.0005889148825579989,\n",
       "  0.0005795122789485114,\n",
       "  0.0005694625350380582,\n",
       "  0.0005587574601772108,\n",
       "  0.0005465844878926873,\n",
       "  0.0005332736057295863,\n",
       "  0.0005186069465707988,\n",
       "  0.0005029424584271121,\n",
       "  0.00048698766373230943,\n",
       "  0.00046949808269606104,\n",
       "  0.0004518382775131613,\n",
       "  0.0004352039457964046,\n",
       "  0.00041732738775733324,\n",
       "  0.0004009274873949055,\n",
       "  0.00038645180341388496,\n",
       "  0.00037183057949213047,\n",
       "  0.00035944774780156355,\n",
       "  0.00035020445440230627,\n",
       "  0.00033929791866934726,\n",
       "  0.00032853305622536155,\n",
       "  0.00031874506385065615,\n",
       "  0.0003164873217299048,\n",
       "  0.0003153325274719724,\n",
       "  0.00030396672913671603,\n",
       "  0.00029203203822752196,\n",
       "  0.00029454189435844976,\n",
       "  0.00028098548604508063,\n",
       "  0.00025804087220292004,\n",
       "  0.0002664397990364315,\n",
       "  0.00026958033725220176,\n",
       "  0.00023243252404167185,\n",
       "  0.00022964564725823169,\n",
       "  0.0002787272075823109,\n",
       "  0.00022095123339178308,\n",
       "  0.00021219382428431084,\n",
       "  0.00026549058176377524,\n",
       "  0.00020607186265156737,\n",
       "  0.00019412552813134556,\n",
       "  0.00023103102284949273,\n",
       "  0.00019344881625979075,\n",
       "  0.00018120248777060105,\n",
       "  0.0002060218924140957,\n",
       "  0.00017105067464789108,\n",
       "  0.00016703598521417007,\n",
       "  0.00017556659440742806,\n",
       "  0.00016108809774907838,\n",
       "  0.00015070282825035974,\n",
       "  0.0001515922007716394,\n",
       "  0.00014147142896295657,\n",
       "  0.00013918993395886252,\n",
       "  0.00013689599706724818,\n",
       "  0.00013374780142580027,\n",
       "  0.00013299108832143247,\n",
       "  0.00013289905889126073,\n",
       "  0.00012900706808847775,\n",
       "  0.00012425949327215285,\n",
       "  0.00012566494323047145,\n",
       "  0.0001241332689200395,\n",
       "  0.00012251984194985459,\n",
       "  0.00012587847387684242,\n",
       "  0.00012593205298929076,\n",
       "  0.00012007635502543832,\n",
       "  0.00012822014157011705,\n",
       "  0.00011859214022738993,\n",
       "  0.00012661097727167153,\n",
       "  0.0001175625147880055,\n",
       "  0.00012110146105572182,\n",
       "  0.00011741570987006915,\n",
       "  0.00011965729364809314,\n",
       "  0.00011787427553956929,\n",
       "  0.00011594434961027998,\n",
       "  0.00012253513289449205,\n",
       "  0.00011517619246400759,\n",
       "  0.00011350276430935733,\n",
       "  0.00012709492979671007,\n",
       "  0.00011958982109458052,\n",
       "  0.00011821185450701575,\n",
       "  0.00012424152269626835,\n",
       "  0.00011709591802043308,\n",
       "  0.00011420445246455659,\n",
       "  0.00011326031173146995,\n",
       "  0.00011365997228754818,\n",
       "  0.00011261420773475297,\n",
       "  0.00011283836959462081,\n",
       "  0.0001111747998428265,\n",
       "  0.00011206706975437035,\n",
       "  0.00011172697642385694,\n",
       "  0.00011218660282403497,\n",
       "  0.00011172279066938375,\n",
       "  0.00010997220981932645],\n",
       " 'f2_history': [0.008592541673827118,\n",
       "  0.008765580111747008,\n",
       "  0.007040270346381301,\n",
       "  0.008351293103448275,\n",
       "  0.009394668971873146,\n",
       "  0.00882582489855164,\n",
       "  0.00915436899148858,\n",
       "  0.009353831720379067,\n",
       "  0.009732394168965566,\n",
       "  0.009794004784612174,\n",
       "  0.009973842917795354,\n",
       "  0.010543844212883851,\n",
       "  0.01162847374541302,\n",
       "  0.01289329685362517,\n",
       "  0.014226397113879246,\n",
       "  0.01616556909115137,\n",
       "  0.018109747161410537,\n",
       "  0.020155464879454816,\n",
       "  0.023088654785776822,\n",
       "  0.025892133723183147,\n",
       "  0.02928559932261744,\n",
       "  0.032733400258138534,\n",
       "  0.035684815495751115,\n",
       "  0.03871345029239766,\n",
       "  0.04761138073004117,\n",
       "  0.06169959644498355,\n",
       "  0.07968815533022476,\n",
       "  0.11235955056179775,\n",
       "  0.1572700296735905,\n",
       "  0.2042380522993688,\n",
       "  0.2405683895629051,\n",
       "  0.26644493717664447,\n",
       "  0.29487824150563896,\n",
       "  0.30027932960893855,\n",
       "  0.3229272706707386,\n",
       "  0.36449490662139217,\n",
       "  0.38808109768584886,\n",
       "  0.40164632415554924,\n",
       "  0.3908931112413158,\n",
       "  0.4295913343180699,\n",
       "  0.45781637717121587,\n",
       "  0.48191308570041275,\n",
       "  0.5037618580307491,\n",
       "  0.5183855526544822,\n",
       "  0.5541267243394903,\n",
       "  0.5547615954267897,\n",
       "  0.5712202236266407,\n",
       "  0.5942926369632521,\n",
       "  0.6011000523834469,\n",
       "  0.6036242981112813,\n",
       "  0.6154974659909309,\n",
       "  0.6279683377308707,\n",
       "  0.617686907254588,\n",
       "  0.6367707098643487,\n",
       "  0.6576492537313433,\n",
       "  0.6642495553427281,\n",
       "  0.6729073395740063,\n",
       "  0.6924850786760717,\n",
       "  0.6953507340946167,\n",
       "  0.7141892804104226,\n",
       "  0.7166035694970254,\n",
       "  0.726962457337884,\n",
       "  0.7388999337309476,\n",
       "  0.7473118279569892,\n",
       "  0.7476893364602014,\n",
       "  0.7545271629778671,\n",
       "  0.7648856990962254,\n",
       "  0.766724298443942,\n",
       "  0.7721620537505014,\n",
       "  0.7741505662891406,\n",
       "  0.7752255947497949,\n",
       "  0.7808310991957105,\n",
       "  0.7857239788302348,\n",
       "  0.7857429450314297,\n",
       "  0.7876619870410367,\n",
       "  0.7876106194690266,\n",
       "  0.7885928504485206,\n",
       "  0.7894736842105263,\n",
       "  0.7956551255940258,\n",
       "  0.7907326407873154,\n",
       "  0.795921160689844,\n",
       "  0.7940936309199782,\n",
       "  0.8006558272988114,\n",
       "  0.7970623905134079,\n",
       "  0.801490514905149,\n",
       "  0.8001874916298379,\n",
       "  0.8014695876990067,\n",
       "  0.8040098199672667,\n",
       "  0.807309423155598,\n",
       "  0.8044737905942595,\n",
       "  0.805976149001742,\n",
       "  0.8021354237059062,\n",
       "  0.797130870212478,\n",
       "  0.7997338656021291,\n",
       "  0.8108108108108109,\n",
       "  0.8026619584408529,\n",
       "  0.8153347732181425,\n",
       "  0.8078871696563056,\n",
       "  0.8086680761099366,\n",
       "  0.8095952023988006,\n",
       "  0.8102417821244227,\n",
       "  0.8140799130198424,\n",
       "  0.811253755804425,\n",
       "  0.8127112914131169,\n",
       "  0.809497741891337,\n",
       "  0.8145139080143211,\n",
       "  0.8149208947081288],\n",
       " 'model': TemporalEdgeClassifier(\n",
       "   (rnn): GRUCell(15, 128)\n",
       "   (gnn1): SAGEConv(128, 128, aggr=mean)\n",
       "   (gnn2): SAGEConv(128, 128, aggr=mean)\n",
       "   (gnn3): SAGEConv(128, 128, aggr=mean)\n",
       "   (dropout): Dropout(p=0.3, inplace=False)\n",
       "   (classifier): Sequential(\n",
       "     (0): Linear(in_features=265, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Dropout(p=0.3, inplace=False)\n",
       "     (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82e25fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Function to compute and print confusion matrix\n",
    "def compute_confusion_matrix(labels, preds, threshold=0.5):\n",
    "\n",
    "    # Convert probabilities to binary predictions using the threshold\n",
    "    binary_preds = (preds >= threshold).astype(int)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(labels, binary_preds)\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Optional: Extract and print TP, TN, FP, FN\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"False Negatives (FN): {fn}\")\n",
    "    print(f\"True Positives (TP): {tp}\")\n",
    "    print(f\"Precision: {tp / (tp + fp + 1e-8):.4f}\")\n",
    "    print(f\"Recall: {tp / (tp + fn + 1e-8):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51d19dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = results['test_probs']\n",
    "test_labels = results['test_labels']\n",
    "val_score = results['val_probs']\n",
    "val_labels = results['val_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f1f36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1456922     440]\n",
      " [    243    1215]]\n",
      "True Negatives (TN): 1456922\n",
      "False Positives (FP): 440\n",
      "False Negatives (FN): 243\n",
      "True Positives (TP): 1215\n",
      "Precision: 0.7341\n",
      "Recall: 0.8333\n"
     ]
    }
   ],
   "source": [
    "compute_confusion_matrix(val_labels, val_score, threshold=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c31c4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1382693     475]\n",
      " [    276    1365]]\n",
      "True Negatives (TN): 1382693\n",
      "False Positives (FP): 475\n",
      "False Negatives (FN): 276\n",
      "True Positives (TP): 1365\n",
      "Precision: 0.7418\n",
      "Recall: 0.8318\n"
     ]
    }
   ],
   "source": [
    "compute_confusion_matrix(test_labels, test_score, threshold=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b1255f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR AUC Score: 0.8562\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate PR curve data\n",
    "precision, recall, thresholds = precision_recall_curve(test_labels, test_score)\n",
    "pr_auc = average_precision_score(test_labels, test_score)\n",
    "\n",
    "# Plot PR curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, linewidth=2, label=f'PR AUC = {pr_auc:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"PR AUC Score: {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c5373b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save precision, recall, thresholds\n",
    "np.savez('pr_curve_data.npz', precision=precision, recall=recall, thresholds=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d33b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-backup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
