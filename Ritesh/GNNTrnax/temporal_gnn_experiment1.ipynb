{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83228cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Temporal Graph Neural Network for Anti-Money Laundering Detection\n",
    "================================================================\n",
    "\n",
    "This implementation creates a temporal graph model optimized for detecting\n",
    "money laundering patterns in the SAML-D dataset with focus on improving recall\n",
    "while maintaining precision.\n",
    "\n",
    "Key Features:\n",
    "- Multi-scale temporal modeling (hourly, daily, weekly)\n",
    "- Dynamic node embeddings with memory mechanisms\n",
    "- Attention-based temporal aggregation\n",
    "- Class imbalance handling for 0.15% positive class\n",
    "- Scalable architecture for 9.5M transactions\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, f1_score\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f74ecce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent.parent))  # Adjust as needed\n",
    "from config import DATAPATH, SAMPLE_DATAPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ddba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalGraphDataProcessor:\n",
    "    \"\"\"\n",
    "    Processes SAML-D dataset into temporal graph format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, time_window_hours=168):  # 7 days default\n",
    "        self.time_window_hours = time_window_hours\n",
    "        self.scalers = {}\n",
    "        self.encoders = {}\n",
    "        \n",
    "    def load_and_preprocess(self, df):\n",
    "        \"\"\"Load SAML-D dataset and perform initial preprocessing\"\"\"\n",
    "\n",
    "        print(\"Loading data...\")\n",
    "        # Combine date and time into datetime\n",
    "        df['datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "        \n",
    "        # Sort by datetime for temporal processing\n",
    "        df = df.sort_values('datetime').reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Loaded {len(df)} transactions\")\n",
    "        print(f\"Suspicious transactions: {df['Is_laundering'].sum()} ({df['Is_laundering'].mean()*100:.3f}%)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"Create temporal and graph-specific features\"\"\"\n",
    "        print(\"Engineering features...\")\n",
    "        \n",
    "        # Time-based features\n",
    "        df['hour'] = df['datetime'].dt.hour.astype('int8')\n",
    "        df['day_of_week'] = df['datetime'].dt.dayofweek.astype('int8')\n",
    "        df['day_of_month'] = df['datetime'].dt.day.astype('int8')\n",
    "        df['is_weekend'] = (df['day_of_week'] >= 5).astype('int8')\n",
    "\n",
    "        # Log transform amount to handle skewness\n",
    "        df['log_amount'] = np.log1p(df['Amount']).astype('float32')\n",
    "        \n",
    "        # Cross-border indicator\n",
    "        df['is_cross_border'] = (df['Payment_type'] == 'Cross-border').astype('int8')\n",
    "\n",
    "        # Risky countries\n",
    "        risky_countries = {'Mexico', 'Turkey', 'Morocco', 'UAE'}  # Example risky countries\n",
    "        df['sender_high_risk'] = df['Sender_bank_location'].isin(risky_countries).astype('int8')\n",
    "        df['receiver_high_risk'] = df['Receiver_bank_location'].isin(risky_countries).astype('int8')\n",
    "\n",
    "        # Currency mismatch\n",
    "        df['currency_mismatch'] = (df['Payment_currency'] != df['Received_currency']).astype('int8')\n",
    "\n",
    "        # Converting Is_laundering to int8\n",
    "        df['Is_laundering'] = df['Is_laundering'].astype('int8')\n",
    "\n",
    "        # Delete unnecessary columns\n",
    "        df = df.drop(columns=['Date', 'Time', 'Amount', 'Sender_bank_location', 'Receiver_bank_location', \n",
    "                              'Payment_currency', 'Received_currency', 'Laundering_type'])\n",
    "\n",
    "        # Transaction frequency features (last 24h, 7d, 30d)\n",
    "        print(\"Computing temporal features...\")\n",
    "        df = self._compute_temporal_frequencies(df)\n",
    "        \n",
    "        # Fan-in/Fan-out patterns\n",
    "        print(\"Computing network features...\")\n",
    "        df = self._compute_network_features(df)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _compute_temporal_frequencies(self, df, window_hours_list=[24, 168]):\n",
    "        \"\"\"Compute transaction frequencies for different time windows\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        timestamps_numeric = df['datetime'].values.astype('int64') // 10**9\n",
    "        senders = df['Sender_account'].values\n",
    "        receivers = df['Receiver_account'].values\n",
    "\n",
    "        for window_hours in window_hours_list:  # 1 day, 7 days\n",
    "            window_nanoseconds = int(pd.Timedelta(hours=window_hours).total_seconds())\n",
    "            \n",
    "            sender_freq = np.zeros(len(df), dtype=int)\n",
    "            receiver_freq = np.zeros(len(df), dtype=int)\n",
    "\n",
    "            for idx in tqdm(range(len(df))):\n",
    "                current_time_numeric = timestamps_numeric[idx]\n",
    "                start_time_numeric = current_time_numeric - window_nanoseconds\n",
    "                \n",
    "                # Binary search for time window\n",
    "                start_idx = np.searchsorted(timestamps_numeric, start_time_numeric, side='left')\n",
    "                end_idx = idx + 1\n",
    "                \n",
    "                # Count in window\n",
    "                if start_idx < end_idx:\n",
    "                    window_senders = senders[start_idx:end_idx]\n",
    "                    window_receivers = receivers[start_idx:end_idx]\n",
    "                    \n",
    "                    sender_freq[idx] = np.sum(window_senders == senders[idx])\n",
    "                    receiver_freq[idx] = np.sum(window_receivers == receivers[idx])\n",
    "            \n",
    "            df[f'sender_freq_{window_hours}h'] = sender_freq\n",
    "            df[f'receiver_freq_{window_hours}h'] = receiver_freq\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _compute_network_features(self, df):\n",
    "        \"\"\"Vectorized network features calculation\"\"\"\n",
    "        df = df.copy().sort_values('datetime').reset_index(drop=True)\n",
    "        \n",
    "        # Pre-convert to categoricals for faster comparisons\n",
    "        df['Sender_account'] = df['Sender_account'].astype('category')\n",
    "        df['Receiver_account'] = df['Receiver_account'].astype('category')\n",
    "        \n",
    "        timestamps = df['datetime'].astype('int64').values\n",
    "        window_ns = pd.Timedelta(days=30).value\n",
    "        \n",
    "        # Pre-allocate results\n",
    "        n = len(df)\n",
    "        fanout_30d = np.zeros(n, dtype=int)\n",
    "        fanin_30d = np.zeros(n, dtype=int)\n",
    "        back_forth_transfers = np.zeros(n, dtype=int)\n",
    "        \n",
    "        # Convert to numpy arrays for faster access\n",
    "        senders = df['Sender_account'].cat.codes.values\n",
    "        receivers = df['Receiver_account'].cat.codes.values\n",
    "    \n",
    "        for idx in tqdm(range(n)):\n",
    "            current_time = timestamps[idx]\n",
    "            start_time = current_time - window_ns\n",
    "            \n",
    "            # Find window indices\n",
    "            start_idx = np.searchsorted(timestamps, start_time, side='left')\n",
    "            \n",
    "            # Work with numpy arrays instead of pandas\n",
    "            window_senders = senders[start_idx:idx+1]\n",
    "            window_receivers = receivers[start_idx:idx+1]\n",
    "            window_timestamps = timestamps[start_idx:idx+1]\n",
    "            \n",
    "            current_sender = senders[idx]\n",
    "            current_receiver = receivers[idx]\n",
    "            \n",
    "            # Vectorized calculations\n",
    "            sender_mask = window_senders == current_sender\n",
    "            receiver_mask = window_receivers == current_receiver\n",
    "            \n",
    "            fanout_30d[idx] = len(np.unique(window_receivers[sender_mask]))\n",
    "            fanin_30d[idx] = len(np.unique(window_senders[receiver_mask]))\n",
    "            \n",
    "            # Back-forth: both directions between current sender/receiver\n",
    "            back_forth_mask = ((window_senders == current_sender) & \n",
    "                            (window_receivers == current_receiver)) | \\\n",
    "                            ((window_senders == current_receiver) & \n",
    "                            (window_receivers == current_sender))\n",
    "            back_forth_transfers[idx] = np.sum(back_forth_mask)\n",
    "        \n",
    "        df['fanout_30d'] = fanout_30d\n",
    "        df['fanin_30d'] = fanin_30d\n",
    "        df['back_forth_transfers'] = back_forth_transfers\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_temporal_snapshots(self, df):\n",
    "        \"\"\"Create temporal graph snapshots\"\"\"\n",
    "        print(\"Creating temporal graph snapshots...\")\n",
    "        \n",
    "        # Sort by datetime\n",
    "        df = df.sort_values('datetime')\n",
    "\n",
    "        # Get all unique accounts globally\n",
    "        all_accounts = list(set(df['Sender_account'].unique()) | set(df['Receiver_account'].unique()))\n",
    "        global_account_to_idx = {acc: idx for idx, acc in enumerate(all_accounts)}\n",
    "        global_num_nodes = len(all_accounts)\n",
    "        \n",
    "        # Define time windows\n",
    "        start_time = df['datetime'].min().normalize()  # Start of first day\n",
    "        end_time = df['datetime'].max().normalize() + pd.Timedelta(days=1)  # Start of day after last day\n",
    "\n",
    "        snapshots = []\n",
    "        current_time = start_time\n",
    "        print(f\"Total time range: {start_time.date()} to {end_time.date()}\")\n",
    "\n",
    "        while current_time < end_time:\n",
    "            window_end = current_time + pd.Timedelta(hours=self.time_window_hours)\n",
    "            print(f\"Processing window: {current_time} to {window_end}\")\n",
    "\n",
    "            # Get transactions in current window\n",
    "            window_mask = (df['datetime'] >= current_time) & (df['datetime'] < window_end)\n",
    "            window_data = df[window_mask].copy()\n",
    "            \n",
    "            if len(window_data) > 0:\n",
    "                # Create graph for this window\n",
    "                graph_data = self._create_graph_snapshot(window_data, current_time, global_account_to_idx, global_num_nodes)\n",
    "                if graph_data is not None:\n",
    "                    snapshots.append(graph_data)\n",
    "            \n",
    "            current_time = window_end\n",
    "        \n",
    "        print(f\"Created {len(snapshots)} temporal snapshots\")\n",
    "        return snapshots, global_num_nodes\n",
    "    \n",
    "    def _create_graph_snapshot(self, window_data, timestamp, global_account_to_idx, global_num_nodes):\n",
    "        \"\"\"Create a single graph snapshot\"\"\"\n",
    "        if len(window_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Get unique accounts\n",
    "        active_accounts = set(window_data['Sender_account'].tolist() + \n",
    "                           window_data['Receiver_account'].tolist())\n",
    "\n",
    "        # Create edges (transactions)\n",
    "        feature_columns = [\n",
    "            'log_amount', 'hour', 'day_of_week', 'is_weekend',\n",
    "            'is_cross_border', 'currency_mismatch',\n",
    "            'sender_high_risk', 'receiver_high_risk'\n",
    "        ]\n",
    "\n",
    "        sender_mapped = window_data['Sender_account'].map(global_account_to_idx)\n",
    "        receiver_mapped = window_data['Receiver_account'].map(global_account_to_idx)\n",
    "        edge_index = np.column_stack((sender_mapped, receiver_mapped))\n",
    "        edge_features = window_data[feature_columns].values\n",
    "        transaction_labels = window_data['Is_laundering'].values\n",
    "        \n",
    "        # Create node features (account features) - zero for all, update active\n",
    "        node_features = np.zeros((global_num_nodes, 5))  # 5 features: sender_freq_24h, receiver_freq_24h, fanout_30d, fanin_30d, back_forth_transfers\n",
    "\n",
    "        for account in active_accounts:\n",
    "            # Get account statistics from window\n",
    "            account_data = window_data[\n",
    "                (window_data['Sender_account'] == account) | \n",
    "                (window_data['Receiver_account'] == account)\n",
    "            ]     \n",
    "            if len(account_data) > 0:\n",
    "                node_feat = [\n",
    "                    account_data['sender_freq_24h'].max() if 'sender_freq_24h' in account_data.columns else 0,\n",
    "                    account_data['receiver_freq_24h'].max() if 'receiver_freq_24h' in account_data.columns else 0,\n",
    "                    account_data['fanout_30d'].max() if 'fanout_30d' in account_data.columns else 0,\n",
    "                    account_data['fanin_30d'].max() if 'fanin_30d' in account_data.columns else 0,\n",
    "                    account_data['back_forth_transfers'].max() if 'back_forth_transfers' in account_data.columns else 0,\n",
    "                ]\n",
    "                global_idx = global_account_to_idx[account]\n",
    "                node_features[global_idx] = node_feat\n",
    "        \n",
    "        # Convert to tensors\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "        edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
    "        transaction_labels = torch.tensor(transaction_labels, dtype=torch.float)\n",
    "        \n",
    "        # Create PyTorch Geometric data object\n",
    "        data = Data(\n",
    "            x=node_features,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_features,\n",
    "            y=transaction_labels,\n",
    "            timestamp=timestamp,\n",
    "            num_nodes=global_num_nodes\n",
    "        )\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fbfa562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal GNN Model for Edge Classification\n",
    "class TemporalEdgeClassifier(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, hidden_dim):\n",
    "        super(TemporalEdgeClassifier, self).__init__()\n",
    "        self.rnn = nn.GRUCell(node_dim, hidden_dim)\n",
    "        self.gnn1 = GATConv(hidden_dim, hidden_dim)\n",
    "        self.gnn2 = GATConv(hidden_dim, hidden_dim)\n",
    "        self.lin = nn.Linear(hidden_dim * 2 + edge_dim, 1)  # Binary classification\n",
    "\n",
    "    def forward(self, data, h):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "        \n",
    "        # Update node hidden states with RNN (using current x)\n",
    "        h = self.rnn(x, h)\n",
    "        \n",
    "        # Apply GNN layers\n",
    "        h = F.relu(self.gnn1(h, edge_index))\n",
    "        h = F.relu(self.gnn2(h, edge_index))\n",
    "        \n",
    "        # Edge features: concat sender h, receiver h, edge_attr\n",
    "        h_i = h[edge_index[0]]\n",
    "        h_j = h[edge_index[1]]\n",
    "        edge_input = torch.cat([h_i, h_j, edge_attr], dim=-1)\n",
    "        \n",
    "        # Prediction\n",
    "        out = self.lin(edge_input)\n",
    "        \n",
    "        return out, h  # Return logits and updated h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ff59662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(snapshots, global_num_nodes, epochs=50, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    node_dim = 5  # From node features\n",
    "    edge_dim = 8  # From edge features: log_amount, hour, day_of_week, is_weekend, is_cross_border, currency_mismatch, sender_high_risk, receiver_high_risk\n",
    "    hidden_dim = 64\n",
    "    \n",
    "    model = TemporalEdgeClassifier(node_dim, edge_dim, hidden_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Compute pos_weight for imbalance (~960:1 from ~0.1% positive)\n",
    "    total_trans = sum(len(s.y) for s in snapshots)\n",
    "    total_pos = sum(s.y.sum() for s in snapshots)\n",
    "    pos_weight = torch.tensor([(total_trans - total_pos) / total_pos]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    # Split snapshots chronologically: 70% train, 15% val, 15% test\n",
    "    n = len(snapshots)\n",
    "    train_end = int(0.7 * n)\n",
    "    val_end = int(0.85 * n)\n",
    "    train_snaps = snapshots[:train_end]\n",
    "    val_snaps = snapshots[train_end:val_end]\n",
    "    test_snaps = snapshots[val_end:]\n",
    "    \n",
    "    print(f\"Training on {len(train_snaps)} snapshots, validating on {len(val_snaps)}, testing on {len(test_snaps)}\")\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    val_preds, val_labels = [], []\n",
    "    test_preds, test_labels = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        h = torch.zeros(global_num_nodes, hidden_dim).to(device)  # Initial hidden state\n",
    "        \n",
    "        train_loss = 0\n",
    "        for snap in train_snaps:\n",
    "            snap = snap.to(device)\n",
    "            out, h = model(snap, h)\n",
    "            loss = criterion(out.squeeze(), snap.y)\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            h = h.detach()  # Detach history\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_snaps)\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            h = torch.zeros(global_num_nodes, hidden_dim).to(device)\n",
    "            val_loss = 0\n",
    "            for snap in val_snaps:\n",
    "                snap = snap.to(device)\n",
    "                out, h = model(snap, h)\n",
    "                loss = criterion(out.squeeze(), snap.y)\n",
    "                val_loss += loss.item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_snaps)\n",
    "            val_loss_history.append(avg_val_loss)\n",
    "        \n",
    "        # Print loss every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"After Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training complete. Generating predictions...\")\n",
    "    # Validation set    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        h = torch.zeros(global_num_nodes, hidden_dim).to(device)\n",
    "        for snap in val_snaps:\n",
    "            snap = snap.to(device)\n",
    "            out, h = model(snap, h)\n",
    "            preds = torch.sigmoid(out).squeeze().cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_labels.extend(snap.y.cpu().numpy())\n",
    "\n",
    "    # Test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        h = torch.zeros(global_num_nodes, hidden_dim).to(device)\n",
    "        for snap in test_snaps:\n",
    "            snap = snap.to(device)\n",
    "            out, h = model(snap, h)\n",
    "            preds = torch.sigmoid(out).squeeze().cpu().numpy()\n",
    "            test_preds.extend(preds)\n",
    "            test_labels.extend(snap.y.cpu().numpy())\n",
    "    \n",
    "    return {\n",
    "        'train_loss_history': train_loss_history,\n",
    "        'val_loss_history': val_loss_history,\n",
    "        'val_preds': np.array(val_preds),\n",
    "        'val_labels': np.array(val_labels),\n",
    "        'test_preds': np.array(test_preds),\n",
    "        'test_labels': np.array(test_labels)\n",
    "    }\n",
    "\n",
    "# Usage Example (assuming data is loaded and processed)\n",
    "# processor = TemporalGraphDataProcessor()\n",
    "# df = processor.load_and_preprocess(df)\n",
    "# df = processor.engineer_features(df)\n",
    "# snapshots, global_num_nodes = processor.create_temporal_snapshots(df)\n",
    "# results = train_model(snapshots, global_num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2124555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the entire dataset\n",
    "df = pd.read_csv(DATAPATH)\n",
    "\n",
    "# Filter by data range\n",
    "# df = df[df['Date'] < '2022-12-31']\n",
    "# df = df.head(300000).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2875433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 9504852 transactions\n",
      "Suspicious transactions: 9873 (0.104%)\n",
      "Engineering features...\n",
      "Computing temporal features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9504852/9504852 [07:30<00:00, 21075.47it/s]\n",
      "100%|██████████| 9504852/9504852 [50:29<00:00, 3137.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing network features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9504852/9504852 [4:49:29<00:00, 547.22it/s]  \n"
     ]
    }
   ],
   "source": [
    "processor = TemporalGraphDataProcessor()\n",
    "df = processor.load_and_preprocess(df)\n",
    "df = processor.engineer_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2bc437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('tmp/processed_data_tgnn.csv')\n",
    "df.to_csv('tmp/processed_data_tgnn_full.csv', index=False)\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "387b70cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporal graph snapshots...\n",
      "Total time range: 2022-10-07 to 2022-12-31\n",
      "Processing window: 2022-10-07 00:00:00 to 2022-10-14 00:00:00\n",
      "Processing window: 2022-10-14 00:00:00 to 2022-10-21 00:00:00\n",
      "Processing window: 2022-10-21 00:00:00 to 2022-10-28 00:00:00\n",
      "Processing window: 2022-10-28 00:00:00 to 2022-11-04 00:00:00\n",
      "Processing window: 2022-11-04 00:00:00 to 2022-11-11 00:00:00\n",
      "Processing window: 2022-11-11 00:00:00 to 2022-11-18 00:00:00\n",
      "Processing window: 2022-11-18 00:00:00 to 2022-11-25 00:00:00\n",
      "Processing window: 2022-11-25 00:00:00 to 2022-12-02 00:00:00\n",
      "Processing window: 2022-12-02 00:00:00 to 2022-12-09 00:00:00\n",
      "Processing window: 2022-12-09 00:00:00 to 2022-12-16 00:00:00\n",
      "Processing window: 2022-12-16 00:00:00 to 2022-12-23 00:00:00\n",
      "Processing window: 2022-12-23 00:00:00 to 2022-12-30 00:00:00\n",
      "Processing window: 2022-12-30 00:00:00 to 2023-01-06 00:00:00\n",
      "Created 13 temporal snapshots\n"
     ]
    }
   ],
   "source": [
    "snapshots, global_num_nodes = processor.create_temporal_snapshots(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9be9998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 9 snapshots, validating on 2, testing on 2\n",
      "After Epoch 10: Train Loss: 0.6765, Validation Loss: 0.8470\n",
      "After Epoch 20: Train Loss: 0.4913, Validation Loss: 0.6752\n",
      "After Epoch 30: Train Loss: 0.4004, Validation Loss: 0.5848\n",
      "After Epoch 40: Train Loss: 0.3407, Validation Loss: 0.5350\n",
      "After Epoch 50: Train Loss: 0.3011, Validation Loss: 0.5738\n",
      "After Epoch 60: Train Loss: 0.3020, Validation Loss: 0.6525\n",
      "After Epoch 70: Train Loss: 0.3384, Validation Loss: 0.8250\n",
      "After Epoch 80: Train Loss: 0.2510, Validation Loss: 0.5744\n",
      "After Epoch 90: Train Loss: 0.2582, Validation Loss: 0.6248\n",
      "After Epoch 100: Train Loss: 0.2237, Validation Loss: 0.5874\n",
      "Training complete. Generating predictions...\n"
     ]
    }
   ],
   "source": [
    "results = train_model(snapshots, global_num_nodes, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82e25fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Function to compute and print confusion matrix\n",
    "def compute_confusion_matrix(labels, preds, threshold=0.5):\n",
    "\n",
    "    # Convert probabilities to binary predictions using the threshold\n",
    "    binary_preds = (preds >= threshold).astype(int)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(labels, binary_preds)\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Optional: Extract and print TP, TN, FP, FN\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"False Negatives (FN): {fn}\")\n",
    "    print(f\"True Positives (TP): {tp}\")\n",
    "    print(f\"Precision: {tp / (tp + fp + 1e-8):.4f}\")\n",
    "    print(f\"Recall: {tp / (tp + fn + 1e-8):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f1f36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[379668  30472]\n",
      " [    43    484]]\n",
      "True Negatives (TN): 379668\n",
      "False Positives (FP): 30472\n",
      "False Negatives (FN): 43\n",
      "True Positives (TP): 484\n",
      "Precision: 0.0156\n",
      "Recall: 0.9184\n"
     ]
    }
   ],
   "source": [
    "compute_confusion_matrix(results['val_labels'], results['val_preds'], threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c31c4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[211391  22034]\n",
      " [    25    121]]\n",
      "True Negatives (TN): 211391\n",
      "False Positives (FP): 22034\n",
      "False Negatives (FN): 25\n",
      "True Positives (TP): 121\n",
      "Precision: 0.0055\n",
      "Recall: 0.8288\n"
     ]
    }
   ],
   "source": [
    "compute_confusion_matrix(results['test_labels'], results['test_preds'], threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1255f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5373b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ae20c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-backup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
