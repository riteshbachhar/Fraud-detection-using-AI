{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e86674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "#NN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.optim import Adam\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))  # Adjust as needed\n",
    "from config import DATAPATH, SAMPLE_DATAPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea7b5447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c98f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eaa4217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6c479a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire dataset\n",
    "df = pd.read_csv(DATAPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2544a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by data range\n",
    "# Node feature saved for 2022-12-31\n",
    "df = df[df['Date'] < '2022-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb3632ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sender_account</th>\n",
       "      <th>Receiver_account</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Payment_currency</th>\n",
       "      <th>Received_currency</th>\n",
       "      <th>Sender_bank_location</th>\n",
       "      <th>Receiver_bank_location</th>\n",
       "      <th>Payment_type</th>\n",
       "      <th>Is_laundering</th>\n",
       "      <th>Laundering_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10:35:19</td>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>8724731955</td>\n",
       "      <td>2769355426</td>\n",
       "      <td>1459.15</td>\n",
       "      <td>UK pounds</td>\n",
       "      <td>UK pounds</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "      <td>Cash Deposit</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal_Cash_Deposits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10:35:20</td>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>1491989064</td>\n",
       "      <td>8401255335</td>\n",
       "      <td>6019.64</td>\n",
       "      <td>UK pounds</td>\n",
       "      <td>Dirham</td>\n",
       "      <td>UK</td>\n",
       "      <td>UAE</td>\n",
       "      <td>Cross-border</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal_Fan_Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10:35:20</td>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>287305149</td>\n",
       "      <td>4404767002</td>\n",
       "      <td>14328.44</td>\n",
       "      <td>UK pounds</td>\n",
       "      <td>UK pounds</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "      <td>Cheque</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal_Small_Fan_Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10:35:21</td>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>5376652437</td>\n",
       "      <td>9600420220</td>\n",
       "      <td>11895.00</td>\n",
       "      <td>UK pounds</td>\n",
       "      <td>UK pounds</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "      <td>ACH</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal_Fan_In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10:35:21</td>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>9614186178</td>\n",
       "      <td>3803336972</td>\n",
       "      <td>115.25</td>\n",
       "      <td>UK pounds</td>\n",
       "      <td>UK pounds</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "      <td>Cash Deposit</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal_Cash_Deposits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        Date  Sender_account  Receiver_account    Amount  \\\n",
       "0  10:35:19  2022-10-07      8724731955        2769355426   1459.15   \n",
       "1  10:35:20  2022-10-07      1491989064        8401255335   6019.64   \n",
       "2  10:35:20  2022-10-07       287305149        4404767002  14328.44   \n",
       "3  10:35:21  2022-10-07      5376652437        9600420220  11895.00   \n",
       "4  10:35:21  2022-10-07      9614186178        3803336972    115.25   \n",
       "\n",
       "  Payment_currency Received_currency Sender_bank_location  \\\n",
       "0        UK pounds         UK pounds                   UK   \n",
       "1        UK pounds            Dirham                   UK   \n",
       "2        UK pounds         UK pounds                   UK   \n",
       "3        UK pounds         UK pounds                   UK   \n",
       "4        UK pounds         UK pounds                   UK   \n",
       "\n",
       "  Receiver_bank_location  Payment_type  Is_laundering       Laundering_type  \n",
       "0                     UK  Cash Deposit              0  Normal_Cash_Deposits  \n",
       "1                    UAE  Cross-border              0        Normal_Fan_Out  \n",
       "2                     UK        Cheque              0  Normal_Small_Fan_Out  \n",
       "3                     UK           ACH              0         Normal_Fan_In  \n",
       "4                     UK  Cash Deposit              0  Normal_Cash_Deposits  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51d1837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add and delete columns\n",
    "df['DateTime'] = pd.to_datetime(df[\"Date\"] + ' ' + df[\"Time\"], format='%Y-%m-%d %H:%M:%S')\n",
    "df['Hour'] = pd.to_datetime(df['DateTime']).dt.hour\n",
    "\n",
    "df['Date_Year'] = pd.to_datetime(df['DateTime']).dt.year\n",
    "df['Date_Month'] = pd.to_datetime(df['DateTime']).dt.month\n",
    "df['Date_Day'] = pd.to_datetime(df['DateTime']).dt.day\n",
    "\n",
    "# df.drop(columns=['Laundering_type'], inplace=True)\n",
    "df.drop(columns=['Time', 'Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4395ff55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suspicious: 2386 (0.10%)\n",
      "Normal: 2478422\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "print(f\"Suspicious: {df['Is_laundering'].sum()} ({100*df['Is_laundering'].mean():.2f}%)\")\n",
    "print(f\"Normal: {(~df['Is_laundering'].astype(bool)).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b3c41bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accounts (nodes): 477061\n"
     ]
    }
   ],
   "source": [
    "label_encoders = {}\n",
    "categorical_cols = ['Payment_type', 'Sender_bank_location', 'Receiver_bank_location']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# high risk countries\n",
    "high_risk_countries = ['Mexico', 'Turkey', 'Morocco', 'UAE']  # Replace with actual countries\n",
    "df['high_risk_country'] = (\n",
    "    df['Sender_bank_location'].isin(high_risk_countries) | \n",
    "    df['Receiver_bank_location'].isin(high_risk_countries)\n",
    ").astype(int)\n",
    "\n",
    "# cross border transaction\n",
    "df['cross_border'] = df['Payment_type'].apply(lambda x: 1 if x == 'Cross-border' else 0)    \n",
    "\n",
    "# Create account mapping (account name -> number)\n",
    "all_accounts = pd.concat([df['Sender_account'], df['Receiver_account']]).unique()\n",
    "account_to_idx = {acc: idx for idx, acc in enumerate(all_accounts)}\n",
    "print(f\"Total accounts (nodes): {len(all_accounts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57aa87cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transactions (edges): 2480808\n"
     ]
    }
   ],
   "source": [
    "# Build edge index (who sends to whom)\n",
    "edge_index = torch.tensor([\n",
    "    [account_to_idx[sender] for sender in df['Sender_account']],\n",
    "    [account_to_idx[receiver] for receiver in df['Receiver_account']]\n",
    "], dtype=torch.long)\n",
    "\n",
    "print(f\"Total transactions (edges): {edge_index.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f35b691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(477061, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"\\n=== STEP 3: Creating Node Features ===\")\n",
    "\n",
    "# def compute_node_features(df, account_list, account_to_idx):\n",
    "#     \"\"\"Compute features for each account\"\"\"\n",
    "#     features = []\n",
    "\n",
    "#     for account in tqdm(account_list):\n",
    "#         # Get transactions\n",
    "#         sent = df[df['Sender_account'] == account]\n",
    "#         received = df[df['Receiver_account'] == account]\n",
    "        \n",
    "#         # Compute simple statistics\n",
    "#         features.append([\n",
    "#             len(sent),                                    # number of outgoing transactions\n",
    "#             len(received),                                # number of incoming transactions\n",
    "#             sent['Amount'].sum() if len(sent) > 0 else 0, # total sent\n",
    "#             sent['Amount'].max() if len(sent) > 0 else 0, # max sent\n",
    "#             sent['Amount'].min() if len(sent) > 0 else 0, # min sent\n",
    "#             sent['Amount'].std() if len(sent) > 1 else 0, # std dev sent\n",
    "#             received['Amount'].sum() if len(received) > 0 else 0, # total received\n",
    "#             received['Amount'].max() if len(received) > 0 else 0, # max received\n",
    "#             received['Amount'].min() if len(received) > 0 else 0, # min received\n",
    "#             received['Amount'].std() if len(received) > 1 else 0, # std dev received\n",
    "#             sent['Amount'].mean() if len(sent) > 0 else 0, # average sent\n",
    "#             sent['Receiver_account'].nunique() if len(sent) > 0 else 0, # number of receivers\n",
    "#             received['Sender_account'].nunique() if len(received) > 0 else 0, # number of senders\n",
    "#         ])\n",
    "    \n",
    "#     return np.array(features, dtype=np.float32)\n",
    "\n",
    "# node_features = compute_node_features(df, all_accounts, account_to_idx)\n",
    "# print(f\"Node feature shape: {node_features.shape}\")\n",
    "\n",
    "\n",
    "# np.save('node_features.npy', node_features)\n",
    "node_features = np.load('node_features.npy')\n",
    "node_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0f92200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "node_features = scaler.fit_transform(node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bd3f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(node_features, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe5657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 4: Creating Edge Features ===\n",
      "Edge feature shape: torch.Size([2480808, 13])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== STEP 4: Creating Edge Features ===\")\n",
    "\n",
    "edge_features = df[['Amount', 'Payment_type_encoded', \n",
    "                    'Sender_bank_location_encoded', \n",
    "                    'Receiver_bank_location_encoded', 'high_risk_country']].values\n",
    "\n",
    "# Add cyclical time features\n",
    "hour = df['Hour'].values\n",
    "edge_features = np.hstack((edge_features, np.sin(2 * np.pi * hour.reshape(-1, 1)/24)))\n",
    "edge_features = np.hstack((edge_features, np.cos(2 * np.pi * hour.reshape(-1, 1)/24)))\n",
    "\n",
    "month = df['Date_Month'].values\n",
    "edge_features = np.hstack((edge_features, np.sin(2 * np.pi * month.reshape(-1, 1)/12)))\n",
    "edge_features = np.hstack((edge_features, np.cos(2 * np.pi * month.reshape(-1, 1)/12)))\n",
    "\n",
    "day = df['Date_Day'].values\n",
    "edge_features = np.hstack((edge_features, np.sin(2 * np.pi * day.reshape(-1, 1)/31)))\n",
    "edge_features = np.hstack((edge_features, np.cos(2 * np.pi * day.reshape(-1, 1)/31)))\n",
    "\n",
    "minute = df['DateTime'].dt.minute.values\n",
    "edge_features = np.hstack((edge_features, np.sin(2 * np.pi * minute.reshape(-1, 1)/60)))\n",
    "edge_features = np.hstack((edge_features, np.cos(2 * np.pi * minute.reshape(-1, 1)/60)))\n",
    "\n",
    "edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "\n",
    "print(f\"Edge feature shape: {edge_attr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d691631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 5: Creating Labels ===\n",
      "Labels shape: torch.Size([2480808])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== STEP 5: Creating Labels ===\")\n",
    "\n",
    "y = torch.tensor(df['Is_laundering'].values, dtype=torch.long)\n",
    "print(f\"Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1794e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete dataframe to save memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74274f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 6: Creating Graph ===\n",
      "Graph created:\n",
      "  Nodes: 477061\n",
      "  Edges: 2480808\n",
      "  Node features: 13\n",
      "  Edge features: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== STEP 6: Creating Graph ===\")\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "data = data.to(device)\n",
    "\n",
    "print(f\"Graph created:\")\n",
    "print(f\"  Nodes: {data.num_nodes}\")\n",
    "print(f\"  Edges: {data.num_edges}\")\n",
    "print(f\"  Node features: {data.x.shape[1]}\")\n",
    "print(f\"  Edge features: {data.edge_attr.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41af9d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 7: Splitting Data Temporally ===\n",
      "\n",
      "Temporal Split:\n",
      "  Train: indices 0 to 1984645\n",
      "  Val:   indices 1984646 to 2232725\n",
      "  Test:  indices 2232726 to 2480807\n",
      "\n",
      "Split sizes:\n",
      "  Train: 1984646 (80.0%)\n",
      "  Val: 248080 (10.0%)\n",
      "  Test: 248082 (10.0%)\n",
      "\n",
      "Suspicious transactions per split:\n",
      "  Train: 1926/1984646 (0.097%)\n",
      "  Val: 306/248080 (0.123%)\n",
      "  Test: 154/248082 (0.062%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== STEP 7: Splitting Data Temporally ===\")\n",
    "\n",
    "num_edges = data.num_edges\n",
    "\n",
    "# TEMPORAL SPLIT: Train on past, validate on recent, test on future\n",
    "# Split indices: 70% train, 15% val, 15% test\n",
    "train_size = int(0.80 * num_edges)\n",
    "val_size = int(0.10 * num_edges)\n",
    "\n",
    "train_idx = np.arange(0, train_size)\n",
    "val_idx = np.arange(train_size, train_size + val_size)\n",
    "test_idx = np.arange(train_size + val_size, num_edges)\n",
    "\n",
    "print(f\"\\nTemporal Split:\")\n",
    "print(f\"  Train: indices 0 to {train_size-1}\")\n",
    "print(f\"  Val:   indices {train_size} to {train_size + val_size - 1}\")\n",
    "print(f\"  Test:  indices {train_size + val_size} to {num_edges-1}\")\n",
    "\n",
    "# Create masks\n",
    "train_mask = torch.zeros(num_edges, dtype=torch.bool, device=device)\n",
    "val_mask = torch.zeros(num_edges, dtype=torch.bool, device=device)\n",
    "test_mask = torch.zeros(num_edges, dtype=torch.bool, device=device)\n",
    "\n",
    "train_mask[train_idx] = True\n",
    "val_mask[val_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"  Train: {train_mask.sum()} ({100*train_mask.sum()/num_edges:.1f}%)\")\n",
    "print(f\"  Val: {val_mask.sum()} ({100*val_mask.sum()/num_edges:.1f}%)\")\n",
    "print(f\"  Test: {test_mask.sum()} ({100*test_mask.sum()/num_edges:.1f}%)\")\n",
    "\n",
    "# Check class distribution in each split\n",
    "print(f\"\\nSuspicious transactions per split:\")\n",
    "print(f\"  Train: {data.y[train_mask].sum()}/{train_mask.sum()} ({100*data.y[train_mask].float().mean():.3f}%)\")\n",
    "print(f\"  Val: {data.y[val_mask].sum()}/{val_mask.sum()} ({100*data.y[val_mask].float().mean():.3f}%)\")\n",
    "print(f\"  Test: {data.y[test_mask].sum()}/{test_mask.sum()} ({100*data.y[test_mask].float().mean():.3f}%)\")\n",
    "\n",
    "# Warning if test set has very different distribution\n",
    "train_suspicious_rate = data.y[train_mask].float().mean()\n",
    "test_suspicious_rate = data.y[test_mask].float().mean()\n",
    "\n",
    "if abs(train_suspicious_rate - test_suspicious_rate) > 0.001:  # More than 0.1% difference\n",
    "    print(\"\\n⚠ WARNING: Test set has different suspicious rate than training!\")\n",
    "    print(\"  This is realistic (fraud patterns change over time)\")\n",
    "    print(\"  But it may affect performance metrics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2482244b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 8: Defining Model ===\n",
      "SimpleGAT(\n",
      "  (node_encoder): Linear(in_features=13, out_features=64, bias=True)\n",
      "  (edge_encoder): Linear(in_features=13, out_features=64, bias=True)\n",
      "  (gat1): GATConv(64, 64, heads=4)\n",
      "  (gat2): GATConv(256, 64, heads=4)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=192, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 143,298\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== STEP 8: Defining Model ===\")\n",
    "\n",
    "class SimpleGAT(nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, hidden_dim=64, num_heads=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encode node and edge features\n",
    "        self.node_encoder = nn.Linear(num_node_features, hidden_dim)\n",
    "        self.edge_encoder = nn.Linear(num_edge_features, hidden_dim)\n",
    "        \n",
    "        # GAT layers\n",
    "        self.gat1 = GATConv(hidden_dim, hidden_dim, heads=num_heads, concat=True, edge_dim=hidden_dim)\n",
    "        self.gat2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, concat=False, edge_dim=hidden_dim)\n",
    "        \n",
    "        # Classifier for edges\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 2)  # 2 classes: normal, suspicious\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Encode\n",
    "        x = F.relu(self.node_encoder(x))\n",
    "        edge_embed = F.relu(self.edge_encoder(edge_attr))\n",
    "        \n",
    "        # GAT layer 1\n",
    "        x = self.gat1(x, edge_index, edge_attr=edge_embed)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "\n",
    "        # GAT layer 2\n",
    "        x = self.gat2(x, edge_index, edge_attr=edge_embed)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # For each edge, get sender and receiver embeddings\n",
    "        sender_emb = x[edge_index[0]]\n",
    "        receiver_emb = x[edge_index[1]]\n",
    "        \n",
    "        # Combine with edge features\n",
    "        edge_features = torch.cat([sender_emb, receiver_emb, edge_embed], dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        out = self.classifier(edge_features)\n",
    "        return out\n",
    "    \n",
    "# Create model\n",
    "model = SimpleGAT(\n",
    "    num_node_features=data.x.shape[1],\n",
    "    num_edge_features=data.edge_attr.shape[1],\n",
    "    hidden_dim=64,\n",
    "    num_heads=4\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b815f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 9: Training Setup ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: Normal=0.0010, Suspicious=0.9990\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== STEP 9: Training Setup ===\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)\n",
    "\n",
    "# Loss function (weighted for class imbalance)\n",
    "class_counts = torch.bincount(data.y)\n",
    "class_weights = 1.0 / class_counts.float()\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "print(f\"Class weights: Normal={class_weights[0]:.4f}, Suspicious={class_weights[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43a9a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e205928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 10: Training ===\n",
      "Epoch 010 | Loss: 158.3768 | Val Recall: 1.0000 | Val Precision: 0.0012 | Val F1: 0.0025\n",
      "  → New best recall: 1.0000\n",
      "Epoch 020 | Loss: 72.2170 | Val Recall: 0.8791 | Val Precision: 0.0012 | Val F1: 0.0024\n",
      "Epoch 030 | Loss: 54.7298 | Val Recall: 0.4444 | Val Precision: 0.0013 | Val F1: 0.0026\n",
      "Epoch 040 | Loss: 28.4909 | Val Recall: 0.4281 | Val Precision: 0.0013 | Val F1: 0.0027\n",
      "Epoch 050 | Loss: 22.4792 | Val Recall: 0.5425 | Val Precision: 0.0016 | Val F1: 0.0031\n",
      "Epoch 060 | Loss: 29.9950 | Val Recall: 0.9641 | Val Precision: 0.0015 | Val F1: 0.0030\n",
      "Epoch 070 | Loss: 16.3367 | Val Recall: 0.5490 | Val Precision: 0.0018 | Val F1: 0.0036\n",
      "Epoch 080 | Loss: 7.9677 | Val Recall: 0.4771 | Val Precision: 0.0021 | Val F1: 0.0041\n",
      "Epoch 090 | Loss: 11.5234 | Val Recall: 0.8170 | Val Precision: 0.0019 | Val F1: 0.0037\n",
      "Epoch 100 | Loss: 4.4207 | Val Recall: 0.8758 | Val Precision: 0.0018 | Val F1: 0.0035\n",
      "Epoch 110 | Loss: 2.7123 | Val Recall: 0.5163 | Val Precision: 0.0033 | Val F1: 0.0066\n",
      "Epoch 120 | Loss: 1.6630 | Val Recall: 0.2549 | Val Precision: 0.0059 | Val F1: 0.0116\n",
      "Epoch 130 | Loss: 1.2587 | Val Recall: 0.3105 | Val Precision: 0.0041 | Val F1: 0.0082\n",
      "Epoch 140 | Loss: 0.9132 | Val Recall: 0.6340 | Val Precision: 0.0030 | Val F1: 0.0059\n",
      "Epoch 150 | Loss: 0.8508 | Val Recall: 0.3464 | Val Precision: 0.0035 | Val F1: 0.0069\n",
      "Epoch 160 | Loss: 0.6957 | Val Recall: 0.4837 | Val Precision: 0.0042 | Val F1: 0.0083\n",
      "Epoch 170 | Loss: 0.6103 | Val Recall: 0.4412 | Val Precision: 0.0041 | Val F1: 0.0082\n",
      "Epoch 180 | Loss: 0.6112 | Val Recall: 0.3333 | Val Precision: 0.0046 | Val F1: 0.0090\n",
      "Epoch 190 | Loss: 0.6070 | Val Recall: 0.6438 | Val Precision: 0.0035 | Val F1: 0.0069\n",
      "Epoch 200 | Loss: 0.5958 | Val Recall: 0.7876 | Val Precision: 0.0027 | Val F1: 0.0053\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== STEP 10: Training ===\")\n",
    "\n",
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def evaluate(mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index, data.edge_attr)\n",
    "        pred = out[mask].argmax(dim=1)\n",
    "        \n",
    "        y_true = data.y[mask].cpu().numpy()\n",
    "        y_pred = pred.cpu().numpy()\n",
    "        \n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        \n",
    "        return recall, precision, f1\n",
    "    \n",
    "#Training loop\n",
    "best_val_recall = 0\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train_one_epoch()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        train_recall, train_precision, train_f1 = evaluate(train_mask)\n",
    "        val_recall, val_precision, val_f1 = evaluate(val_mask)\n",
    "        \n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | \"\n",
    "              f\"Val Recall: {val_recall:.4f} | Val Precision: {val_precision:.4f} | Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_recall > best_val_recall:\n",
    "            best_val_recall = val_recall\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            patience_counter = 0\n",
    "            print(f\"  → New best recall: {best_val_recall:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ab9af92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 11: Final Evaluation ===\n",
      "\n",
      "Test Set Results:\n",
      "  Recall:    0.9481\n",
      "  Precision: 0.0054\n",
      "  F1 Score:  0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3551652/2306298191.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "  True Negatives:  220876\n",
      "  False Positives: 27052\n",
      "  False Negatives: 8\n",
      "  True Positives:  146\n",
      "\n",
      "Rates:\n",
      "  TPR (Recall): 0.9481\n",
      "  TNR: 0.8909\n",
      "  FPR: 0.1091\n",
      "  FNR: 0.0519\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== STEP 11: Final Evaluation ===\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_recall, test_precision, test_f1 = evaluate(test_mask)\n",
    "\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  F1 Score:  {test_f1:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    pred = out[test_mask].argmax(dim=1)\n",
    "    \n",
    "    y_true = data.y[test_mask].cpu().numpy()\n",
    "    y_pred = pred.cpu().numpy()\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"  True Negatives:  {tn}\")\n",
    "    print(f\"  False Positives: {fp}\")\n",
    "    print(f\"  False Negatives: {fn}\")\n",
    "    print(f\"  True Positives:  {tp}\")\n",
    "\n",
    "    print(f\"\\nRates:\")\n",
    "    print(f\"  TPR (Recall): {tp/(tp+fn):.4f}\")\n",
    "    print(f\"  TNR: {tn/(tn+fp):.4f}\")\n",
    "    print(f\"  FPR: {fp/(fp+tn):.4f}\")\n",
    "    print(f\"  FNR: {fn/(fn+tp):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc93e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
